---
title: AI Models
description: Comprehensive guide to AI models in Lightfast Chat
---

# AI Models

Lightfast Chat supports multiple AI providers and models, giving you flexibility to choose the best model for your needs.

## Supported Providers

### Anthropic

Anthropic's Claude models are known for their safety, helpfulness, and strong reasoning capabilities.

#### Claude 3.5 Sonnet

```typescript
{
  id: "claude-3-5-sonnet-20241022",
  name: "Claude 3.5 Sonnet",
  provider: "anthropic",
  contextWindow: 200000,
  maxOutput: 8192,
  strengths: [
    "Coding and technical tasks",
    "Creative writing",
    "Complex reasoning",
    "Detailed analysis"
  ],
  pricing: {
    input: "$3 per million tokens",
    output: "$15 per million tokens"
  }
}
```

**Best for:**
- Software development
- Technical documentation
- Complex problem solving
- Creative tasks

#### Claude 3 Opus

```typescript
{
  id: "claude-3-opus-20240229",
  name: "Claude 3 Opus",
  provider: "anthropic",
  contextWindow: 200000,
  maxOutput: 4096,
  strengths: [
    "Highest quality responses",
    "Research and analysis",
    "Complex instructions",
    "Nuanced understanding"
  ],
  pricing: {
    input: "$15 per million tokens",
    output: "$75 per million tokens"
  }
}
```

**Best for:**
- Research tasks
- Complex analysis
- High-stakes decisions
- Quality over speed

#### Claude 3 Haiku

```typescript
{
  id: "claude-3-haiku-20240307",
  name: "Claude 3 Haiku",
  provider: "anthropic",
  contextWindow: 200000,
  maxOutput: 4096,
  strengths: [
    "Fast responses",
    "Cost effective",
    "Simple tasks",
    "High volume processing"
  ],
  pricing: {
    input: "$0.25 per million tokens",
    output: "$1.25 per million tokens"
  }
}
```

**Best for:**
- Quick queries
- Bulk processing
- Cost optimization
- Simple tasks

### OpenAI

OpenAI's GPT models are versatile and widely used across various applications.

#### GPT-4o

```typescript
{
  id: "gpt-4o",
  name: "GPT-4 Omni",
  provider: "openai",
  contextWindow: 128000,
  maxOutput: 4096,
  strengths: [
    "Multimodal capabilities",
    "Fast performance",
    "Broad knowledge",
    "Tool use"
  ],
  pricing: {
    input: "$5 per million tokens",
    output: "$15 per million tokens"
  }
}
```

**Best for:**
- General purpose chat
- Image understanding
- Function calling
- Balanced performance

#### GPT-4 Turbo

```typescript
{
  id: "gpt-4-turbo-preview",
  name: "GPT-4 Turbo",
  provider: "openai",
  contextWindow: 128000,
  maxOutput: 4096,
  strengths: [
    "Latest knowledge",
    "JSON mode",
    "Reproducible outputs",
    "Function calling"
  ],
  pricing: {
    input: "$10 per million tokens",
    output: "$30 per million tokens"
  }
}
```

**Best for:**
- Structured outputs
- API integrations
- Consistent formatting
- Recent information

#### GPT-3.5 Turbo

```typescript
{
  id: "gpt-3.5-turbo",
  name: "GPT-3.5 Turbo",
  provider: "openai",
  contextWindow: 16385,
  maxOutput: 4096,
  strengths: [
    "Very fast",
    "Cost effective",
    "Good for simple tasks",
    "High availability"
  ],
  pricing: {
    input: "$0.50 per million tokens",
    output: "$1.50 per million tokens"
  }
}
```

**Best for:**
- High-speed responses
- Budget constraints
- Simple queries
- High volume

### OpenRouter

Access 100+ models through a unified API.

#### Available Models

```typescript
// Popular models via OpenRouter
const openRouterModels = [
  "meta-llama/llama-3.1-70b-instruct",
  "mistralai/mistral-large",
  "01-ai/yi-large",
  "google/gemini-pro-1.5",
  "cohere/command-r-plus",
  // ... many more
]
```

**Benefits:**
- Single API for multiple providers
- Automatic fallbacks
- Usage-based pricing
- No individual API keys needed

## Model Configuration

### Setting Default Model

```typescript
// lib/models.ts
export const defaultModel = "claude-3-5-sonnet-20241022"

// Or configure per user
export function getUserDefaultModel(userId: string): string {
  const preference = getUserPreference(userId, "defaultModel")
  return preference ?? defaultModel
}
```

### Model Selection Logic

```typescript
// lib/model-selector.ts
export function selectModel(task: TaskType): string {
  switch (task) {
    case "coding":
      return "claude-3-5-sonnet-20241022"
    case "creative":
      return "claude-3-opus-20240229"
    case "quick":
      return "claude-3-haiku-20240307"
    case "data":
      return "gpt-4o"
    default:
      return defaultModel
  }
}
```

### Dynamic Model Switching

```typescript
// components/model-selector.tsx
export function ModelSelector() {
  const [model, setModel] = useState(defaultModel)
  
  return (
    <Select value={model} onValueChange={setModel}>
      <SelectTrigger>
        <SelectValue />
      </SelectTrigger>
      <SelectContent>
        {Object.entries(models).map(([id, config]) => (
          <SelectItem key={id} value={id}>
            <div>
              <div>{config.name}</div>
              <div className="text-xs text-muted-foreground">
                {config.provider} • {config.contextWindow.toLocaleString()} tokens
              </div>
            </div>
          </SelectItem>
        ))}
      </SelectContent>
    </Select>
  )
}
```

## API Integration

### Provider Setup

#### Anthropic Integration

```typescript
// lib/providers/anthropic.ts
import Anthropic from "@anthropic-ai/sdk"

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
})

export async function* streamAnthropicResponse(
  model: string,
  messages: Message[]
) {
  const stream = await anthropic.messages.stream({
    model,
    messages,
    max_tokens: 8192,
  })
  
  for await (const chunk of stream) {
    if (chunk.type === "content_block_delta") {
      yield chunk.delta.text
    }
  }
}
```

#### OpenAI Integration

```typescript
// lib/providers/openai.ts
import OpenAI from "openai"

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

export async function* streamOpenAIResponse(
  model: string,
  messages: Message[]
) {
  const stream = await openai.chat.completions.create({
    model,
    messages,
    stream: true,
  })
  
  for await (const chunk of stream) {
    yield chunk.choices[0]?.delta?.content ?? ""
  }
}
```

#### OpenRouter Integration

```typescript
// lib/providers/openrouter.ts
export async function* streamOpenRouterResponse(
  model: string,
  messages: Message[]
) {
  const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${process.env.OPENROUTER_API_KEY}`,
      "Content-Type": "application/json",
      "HTTP-Referer": process.env.SITE_URL,
    },
    body: JSON.stringify({
      model,
      messages,
      stream: true,
    }),
  })
  
  const reader = response.body?.getReader()
  if (!reader) throw new Error("No response body")
  
  const decoder = new TextDecoder()
  while (true) {
    const { done, value } = await reader.read()
    if (done) break
    
    const chunk = decoder.decode(value)
    // Parse SSE and yield content
    yield parseSSE(chunk)
  }
}
```

### Unified Interface

```typescript
// lib/ai/provider.ts
export interface AIProvider {
  streamResponse(
    model: string,
    messages: Message[],
    options?: StreamOptions
  ): AsyncGenerator<string>
  
  validateModel(model: string): boolean
  getModelConfig(model: string): ModelConfig
}

export class UnifiedAIProvider implements AIProvider {
  private providers: Map<string, AIProvider>
  
  constructor() {
    this.providers = new Map([
      ["anthropic", new AnthropicProvider()],
      ["openai", new OpenAIProvider()],
      ["openrouter", new OpenRouterProvider()],
    ])
  }
  
  async* streamResponse(
    model: string,
    messages: Message[],
    options?: StreamOptions
  ) {
    const config = getModelConfig(model)
    const provider = this.providers.get(config.provider)
    
    if (!provider) {
      throw new Error(`Unknown provider: ${config.provider}`)
    }
    
    yield* provider.streamResponse(model, messages, options)
  }
}
```

## Model Features

### Context Windows

Understanding context limits:

```typescript
// lib/context-manager.ts
export class ContextManager {
  private model: ModelConfig
  
  constructor(model: ModelConfig) {
    this.model = model
  }
  
  canFitMessages(messages: Message[]): boolean {
    const tokenCount = this.estimateTokens(messages)
    return tokenCount < this.model.contextWindow * 0.9 // 90% safety margin
  }
  
  truncateMessages(messages: Message[]): Message[] {
    const maxTokens = this.model.contextWindow * 0.9
    let tokenCount = 0
    const result: Message[] = []
    
    // Keep most recent messages
    for (let i = messages.length - 1; i >= 0; i--) {
      const messageTokens = this.estimateTokens([messages[i]])
      if (tokenCount + messageTokens > maxTokens) break
      
      result.unshift(messages[i])
      tokenCount += messageTokens
    }
    
    return result
  }
  
  private estimateTokens(messages: Message[]): number {
    // Rough estimation: 1 token ≈ 4 characters
    return messages.reduce((sum, msg) => 
      sum + Math.ceil(msg.content.length / 4), 0
    )
  }
}
```

### Special Capabilities

#### Vision Models

```typescript
// Models with image understanding
const visionModels = [
  "gpt-4o",
  "gpt-4-turbo",
  "claude-3-5-sonnet-20241022",
  "claude-3-opus-20240229",
]

// Usage
if (visionModels.includes(model)) {
  messages.push({
    role: "user",
    content: [
      { type: "text", text: "What's in this image?" },
      { type: "image_url", image_url: { url: imageUrl } }
    ]
  })
}
```

#### Function Calling

```typescript
// Models with function calling
const functionModels = [
  "gpt-4o",
  "gpt-4-turbo",
  "gpt-3.5-turbo",
]

// Define functions
const functions = [
  {
    name: "search_web",
    description: "Search the web for information",
    parameters: {
      type: "object",
      properties: {
        query: { type: "string" }
      },
      required: ["query"]
    }
  }
]

// Use with OpenAI
const response = await openai.chat.completions.create({
  model: "gpt-4o",
  messages,
  functions,
  function_call: "auto",
})
```

## Performance Optimization

### Response Streaming

```typescript
// Optimize streaming performance
export class StreamOptimizer {
  private buffer: string = ""
  private lastFlush: number = 0
  
  async* optimize(
    stream: AsyncGenerator<string>
  ): AsyncGenerator<string> {
    for await (const chunk of stream) {
      this.buffer += chunk
      
      // Flush every 100ms or 50 characters
      if (
        Date.now() - this.lastFlush > 100 ||
        this.buffer.length > 50
      ) {
        yield this.buffer
        this.buffer = ""
        this.lastFlush = Date.now()
      }
    }
    
    // Flush remaining
    if (this.buffer) {
      yield this.buffer
    }
  }
}
```

### Caching Strategies

```typescript
// Cache model responses
export class ResponseCache {
  private cache: Map<string, CachedResponse> = new Map()
  
  getCacheKey(model: string, messages: Message[]): string {
    const content = messages.map(m => m.content).join("|")
    return `${model}:${hashString(content)}`
  }
  
  async get(
    model: string, 
    messages: Message[]
  ): Promise<string | null> {
    const key = this.getCacheKey(model, messages)
    const cached = this.cache.get(key)
    
    if (cached && Date.now() - cached.timestamp < 3600000) {
      return cached.response
    }
    
    return null
  }
  
  set(model: string, messages: Message[], response: string) {
    const key = this.getCacheKey(model, messages)
    this.cache.set(key, {
      response,
      timestamp: Date.now(),
    })
  }
}
```

## Cost Management

### Token Counting

```typescript
// Accurate token counting
import { encode } from "@anthropic-ai/tokenizer"

export function countTokens(
  text: string, 
  model: string
): number {
  if (model.includes("claude")) {
    return encode(text).length
  }
  
  // Fallback estimation
  return Math.ceil(text.length / 4)
}
```

### Cost Tracking

```typescript
// Track usage and costs
export class CostTracker {
  async trackUsage(
    userId: string,
    model: string,
    inputTokens: number,
    outputTokens: number
  ) {
    const config = getModelConfig(model)
    const cost = 
      (inputTokens * config.pricing.input) +
      (outputTokens * config.pricing.output)
    
    await db.insert("usage", {
      userId,
      model,
      inputTokens,
      outputTokens,
      cost,
      timestamp: Date.now(),
    })
  }
  
  async getUserUsage(
    userId: string,
    period: "day" | "month"
  ): Promise<UsageSummary> {
    const since = period === "day" 
      ? Date.now() - 86400000
      : Date.now() - 2592000000
    
    const usage = await db
      .query("usage")
      .withIndex("by_user", q => q.eq("userId", userId))
      .filter(q => q.gte(q.field("timestamp"), since))
      .collect()
    
    return {
      totalCost: usage.reduce((sum, u) => sum + u.cost, 0),
      tokenCount: usage.reduce((sum, u) => 
        sum + u.inputTokens + u.outputTokens, 0
      ),
      byModel: groupBy(usage, "model"),
    }
  }
}
```

## Model Selection Guide

### Decision Matrix

| Use Case | Recommended Model | Why |
|----------|------------------|-----|
| **Coding** | Claude 3.5 Sonnet | Best code understanding |
| **Creative Writing** | Claude 3 Opus | Highest quality prose |
| **Quick Answers** | Claude 3 Haiku | Fast and affordable |
| **Data Analysis** | GPT-4o | Structured outputs |
| **Bulk Processing** | GPT-3.5 Turbo | Cost effective |
| **Research** | Claude 3 Opus | Deep analysis |
| **Chat** | Claude 3.5 Sonnet | Balanced performance |

### Fallback Strategy

```typescript
// Implement model fallbacks
export class ModelFallback {
  private fallbackChain = [
    "claude-3-5-sonnet-20241022",
    "gpt-4o",
    "claude-3-haiku-20240307",
    "gpt-3.5-turbo",
  ]
  
  async generateWithFallback(
    messages: Message[],
    preferredModel?: string
  ): Promise<string> {
    const models = preferredModel
      ? [preferredModel, ...this.fallbackChain]
      : this.fallbackChain
    
    for (const model of models) {
      try {
        return await generateResponse(model, messages)
      } catch (error) {
        console.error(`Model ${model} failed:`, error)
        continue
      }
    }
    
    throw new Error("All models failed")
  }
}
```

## Future Models

### Coming Soon

- **Claude 3.5 Opus**: Enhanced reasoning
- **GPT-5**: Next generation capabilities
- **Open Source Models**: Llama 3, Mistral, etc.
- **Specialized Models**: Code, math, science

### Custom Models

```typescript
// Add custom model support
export function registerCustomModel(config: ModelConfig) {
  models[config.id] = config
  
  // Register provider if needed
  if (!providers.has(config.provider)) {
    providers.set(
      config.provider, 
      new CustomProvider(config.endpoint)
    )
  }
}
```

## Best Practices

1. **Model Selection**
   - Choose based on task requirements
   - Consider cost vs quality tradeoffs
   - Use appropriate context windows

2. **Error Handling**
   - Implement retries with exponential backoff
   - Have fallback models ready
   - Monitor rate limits

3. **Performance**
   - Stream responses for better UX
   - Cache common queries
   - Batch similar requests

4. **Cost Control**
   - Track token usage
   - Set user limits
   - Use cheaper models for simple tasks

## Next Steps

- Configure models in [Configuration](/docs/configuration#model-configuration)
- Learn about [API Integration](/docs/api#ai-integration)
- Explore [Self-Hosting Models](/docs/self-hosting#custom-models)