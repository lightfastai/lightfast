---
title: Scaling
description: Scale your agents from prototype to production
---

# Scaling Guide

Learn how to scale your Lightfast agents from handling a few requests to millions of concurrent executions.

## Scaling Dimensions

### Vertical Scaling

Increase resources for individual agents:

```typescript
const verticalConfig = {
  resources: {
    cpu: {
      requests: '2',
      limits: '4'
    },
    memory: {
      requests: '4Gi',
      limits: '8Gi'
    },
    gpu: {
      type: 'nvidia-t4',
      count: 1
    }
  }
};
```

### Horizontal Scaling

Add more agent instances:

```typescript
const horizontalConfig = {
  replicas: {
    min: 5,
    max: 100,
    targetCPU: 70,
    targetMemory: 80,
    scaleUpRate: 5,    // instances per minute
    scaleDownRate: 2   // instances per minute
  }
};
```

## Auto-scaling

### Predictive Scaling

```typescript
const predictiveScaling = {
  enabled: true,
  model: 'timeseries-forecast',
  
  // Learn from historical patterns
  training: {
    lookbackDays: 30,
    features: ['hour', 'dayOfWeek', 'holiday']
  },
  
  // Scale ahead of demand
  prediction: {
    horizonMinutes: 15,
    confidenceThreshold: 0.8
  }
};
```

### Custom Metrics Scaling

```typescript
const customMetrics = {
  metrics: [
    {
      name: 'queue_depth',
      target: 10,
      type: 'average'
    },
    {
      name: 'response_time_p95',
      target: 1000, // ms
      type: 'value'
    },
    {
      name: 'active_sessions',
      target: 50,
      type: 'average'
    }
  ]
};
```

## Load Balancing

### Intelligent Routing

```typescript
class LoadBalancer {
  private instances: AgentInstance[] = [];
  
  async route(request: Request): Promise<Response> {
    // Select instance based on multiple factors
    const instance = this.selectInstance({
      strategy: 'weighted-round-robin',
      factors: {
        cpu: 0.3,
        memory: 0.2,
        activeRequests: 0.3,
        responseTime: 0.2
      }
    });
    
    return instance.handle(request);
  }
  
  private selectInstance(config: LoadBalancerConfig): AgentInstance {
    // Score each instance
    const scores = this.instances.map(instance => ({
      instance,
      score: this.calculateScore(instance, config)
    }));
    
    // Select best instance
    return scores.sort((a, b) => b.score - a.score)[0].instance;
  }
}
```

## Caching Strategies

### Multi-Level Cache

```typescript
class CacheManager {
  private l1Cache = new MemoryCache({ maxSize: '100MB' });
  private l2Cache = new RedisCache({ url: 'redis://cache' });
  private l3Cache = new S3Cache({ bucket: 'cache-bucket' });
  
  async get(key: string): Promise<any> {
    // Check L1 (memory)
    let value = await this.l1Cache.get(key);
    if (value) return value;
    
    // Check L2 (Redis)
    value = await this.l2Cache.get(key);
    if (value) {
      await this.l1Cache.set(key, value);
      return value;
    }
    
    // Check L3 (S3)
    value = await this.l3Cache.get(key);
    if (value) {
      await this.l2Cache.set(key, value);
      await this.l1Cache.set(key, value);
      return value;
    }
    
    return null;
  }
}
```

## Database Optimization

### Connection Pooling

```typescript
const dbConfig = {
  pool: {
    min: 10,
    max: 100,
    acquireTimeout: 30000,
    idleTimeout: 10000,
    reapInterval: 1000
  },
  
  // Read replicas for scaling reads
  replicas: [
    { host: 'replica1.db.com', weight: 1 },
    { host: 'replica2.db.com', weight: 1 },
    { host: 'replica3.db.com', weight: 2 }
  ]
};
```

### Query Optimization

```typescript
class QueryOptimizer {
  async optimizeQuery(query: string): Promise<string> {
    // Add appropriate indexes
    await this.ensureIndexes(query);
    
    // Rewrite for performance
    const optimized = this.rewriteQuery(query);
    
    // Add query hints
    return this.addHints(optimized);
  }
  
  async analyzeSlowQueries() {
    const slowQueries = await this.db.getSlowQueryLog();
    
    for (const query of slowQueries) {
      console.log(`Slow query detected:
        Query: ${query.sql}
        Duration: ${query.duration}ms
        Suggestion: ${this.suggest(query)}`);
    }
  }
}
```

## Rate Limiting

### Distributed Rate Limiting

```typescript
class DistributedRateLimiter {
  private redis: Redis;
  
  async checkLimit(
    key: string,
    limit: number,
    window: number
  ): Promise<boolean> {
    const current = await this.redis.incr(key);
    
    if (current === 1) {
      await this.redis.expire(key, window);
    }
    
    return current <= limit;
  }
  
  async getRemainingQuota(key: string, limit: number): Promise<number> {
    const used = await this.redis.get(key) || 0;
    return Math.max(0, limit - Number(used));
  }
}
```

## Queue Management

### Priority Queues

```typescript
class PriorityQueue {
  async enqueue(task: Task) {
    const priority = this.calculatePriority(task);
    
    await this.redis.zadd(
      'task-queue',
      priority,
      JSON.stringify(task)
    );
  }
  
  private calculatePriority(task: Task): number {
    let score = Date.now();
    
    // Adjust based on factors
    if (task.user.tier === 'premium') score -= 1000000;
    if (task.urgent) score -= 500000;
    if (task.retryCount > 0) score -= 100000;
    
    return score;
  }
}
```

## Batch Processing

### Efficient Batching

```typescript
class BatchProcessor {
  private batch: Request[] = [];
  private batchSize = 100;
  private batchTimeout = 1000; // ms
  
  async add(request: Request): Promise<Response> {
    return new Promise((resolve, reject) => {
      this.batch.push({ request, resolve, reject });
      
      if (this.batch.length >= this.batchSize) {
        this.processBatch();
      } else {
        this.scheduleBatchProcess();
      }
    });
  }
  
  private async processBatch() {
    const currentBatch = this.batch.splice(0, this.batchSize);
    
    try {
      const responses = await this.batchExecute(
        currentBatch.map(b => b.request)
      );
      
      currentBatch.forEach((item, index) => {
        item.resolve(responses[index]);
      });
    } catch (error) {
      currentBatch.forEach(item => item.reject(error));
    }
  }
}
```

## Geographic Distribution

### Multi-Region Setup

```typescript
const multiRegion = {
  regions: {
    'us-east-1': {
      primary: true,
      capacity: 1000
    },
    'eu-west-1': {
      capacity: 500
    },
    'ap-southeast-1': {
      capacity: 300
    }
  },
  
  routing: {
    strategy: 'geoproximity',
    fallback: 'least-latency'
  },
  
  dataReplication: {
    mode: 'async',
    consistency: 'eventual'
  }
};
```

## Performance Testing

### Load Testing

```typescript
import { LoadTest } from '@lightfast/testing';

const loadTest = new LoadTest({
  duration: '10m',
  rampUp: '2m',
  
  scenarios: [
    {
      name: 'normal-load',
      weight: 70,
      rps: 100
    },
    {
      name: 'spike-load',
      weight: 20,
      rps: 500
    },
    {
      name: 'sustained-load',
      weight: 10,
      rps: 200
    }
  ]
});

const results = await loadTest.run();
console.log('P95 Latency:', results.latency.p95);
console.log('Error Rate:', results.errorRate);
```

## Cost Optimization

### Resource Efficiency

```typescript
class ResourceOptimizer {
  async optimizeDeployment() {
    const usage = await this.analyzeUsage();
    
    // Right-size instances
    if (usage.cpu.average < 30) {
      await this.downgradeInstances();
    }
    
    // Use spot instances for batch work
    if (usage.pattern === 'batch') {
      await this.enableSpotInstances();
    }
    
    // Schedule scaling for predictable patterns
    if (usage.pattern === 'predictable') {
      await this.configureScheduledScaling();
    }
  }
}
```

## Best Practices

1. **Start Small**: Begin with vertical scaling, add horizontal as needed
2. **Monitor First**: Understand bottlenecks before scaling
3. **Cache Aggressively**: Reduce load on expensive operations
4. **Async When Possible**: Use queues for non-critical paths
5. **Plan for Failure**: Design for partial system failures
6. **Test at Scale**: Regular load testing prevents surprises

Scale smart, not just big!