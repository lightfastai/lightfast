---
title: Quick Start
description: Install Lightfast and scaffold an agent with one web tool
---

# Quick Start

This guide shows the fastest path to a working agent using the same primitives as our chat app route. You’ll install the package, add one web tool, and expose a typed route with the fetch adapter.

## Prerequisites

- Node 18+
- TypeScript project
- Next.js 14+ using the App Router (`app/` directory)

## What You’ll Build

- A minimal agent that can call a single web tool.
- A Next.js API route that streams responses with `fetchRequestHandler`.
- Optional resumable streams and persisted memory.

## Concepts You’ll Touch

- Agent: Orchestrates the model, tools, and system prompt.
- Tool: A typed function (Zod schema) the model can call.
- Model: Selected via `@ai-sdk/gateway` (e.g., Anthropic, OpenAI).
- Memory: Stores conversation state by `sessionId` (in‑memory or Redis).
- Request handler: `fetchRequestHandler` that accepts AI SDK `messages` and streams a response.

## Flow

- Client → POST `/api/chat/[sessionId]` with `messages`
- Handler → runs agent (model + tools) and streams back
- Memory → saves state by `sessionId` and `resourceId`
- GET → resumes active stream or returns 204 if none

## Install

Use your preferred package manager (Node 18+):

```bash
pnpm add lightfast @ai-sdk/gateway zod
# optional (persisted memory)
pnpm add @upstash/redis
# optional (React hooks for UI)
pnpm add @ai-sdk/react
```

## 1) Define a Single Web Tool

Why: Tools give your agent grounded capabilities. We keep one minimal HTTP tool so you see the end‑to‑end flow without extra complexity.

```ts
// src/tools/web-search.ts
import { createTool } from "lightfast/tool";
import { z } from "zod";

// Minimal web search tool (mock implementation)
export const webSearch = createTool({
  description: "Search the web and return brief results",
  inputSchema: z.object({
    query: z.string().describe("Search query"),
    limit: z.number().int().min(1).max(5).default(3),
  }),
  async execute({ query, limit }) {
    // Replace with your search provider if desired
    // For Quick Start, return mock results deterministically
    return {
      query,
      results: Array.from({ length: limit }).map((_, i) => ({
        title: `Result ${i + 1} for: ${query}`,
        url: `https://example.com/search?q=${encodeURIComponent(query)}#${i + 1}`,
        content: `Snippet ${i + 1} about ${query}.`,
      })),
    };
  },
});
```

## 2) Create an Agent

Why: The agent ties your tool, system prompt, and model together. The `createRuntimeContext` is typed and passed to every tool call, so you can add user/tenant IDs, quotas, or feature flags.

```ts
// src/agents/assistant.ts
import { createAgent } from "lightfast/agent";
import { gateway } from "@ai-sdk/gateway";
import { webSearch } from "../tools/web-search";

// Optional: define runtime context passed to tools
interface AppRuntimeContext {
  userId: string;
  agentId: string;
}

export const assistant = createAgent<AppRuntimeContext, { webSearch: typeof webSearch }>({
  name: "assistant",
  system: "You are a helpful AI assistant. Use tools when helpful.",
  tools: { webSearch },
  model: gateway("anthropic/claude-3-haiku"),
  createRuntimeContext: ({ resourceId }) => ({
    userId: resourceId,
    agentId: "assistant",
  }),
});

Choosing a model:
- Use any model exposed by your AI Gateway (e.g., `openai/gpt-4o-mini`, `anthropic/claude-3-5-sonnet`).
- Configure provider auth per your gateway setup (e.g., `AI_GATEWAY_API_KEY`).
- Start small for local dev; switch to larger models later.
```

## 3) Add a Route with fetchRequestHandler

This mirrors our chat app’s server shape: POST to stream responses, GET to resume (optional).

```ts
// app/api/chat/[sessionId]/route.ts (Next.js App Router)
import { fetchRequestHandler } from "lightfast/server/adapters/fetch";
import { InMemoryMemory } from "lightfast/memory/adapters/in-memory";
import { assistant } from "@/agents/assistant";

// Start with in-memory storage for development
const memory = new InMemoryMemory();

// One handler supports both POST (stream) and GET (resume)
async function handler(
  req: Request,
  { params }: { params: { sessionId: string } }
) {
  const userId = "user-123"; // replace with your auth user id
  return fetchRequestHandler({
    agent: assistant,
    sessionId: params.sessionId,
    memory,
    req,
    resourceId: userId,
    enableResume: true,
    createRequestContext: (request) => ({
      userAgent: request.headers.get("user-agent") ?? undefined,
      ipAddress: request.headers.get("x-forwarded-for") ?? undefined,
    }),
  });
}

export { handler as POST, handler as GET };

Notes:
- Single handler supports both methods; the adapter routes POST vs GET internally.
- `sessionId` groups messages in memory. Use a stable value per chat.
- `resourceId` identifies the actor (usually your authenticated user ID). It’s also used as the owner of the session in memory.
- `createRequestContext` is optional metadata (IP, UA) you can attach for logging/limits.
- GET returns 204 if there’s nothing to resume.
```

### 3b) Client (Optional)

Minimal chat UI using AI SDK v5 transport + React hook (mirrors our chat app pattern of sending only the last user message and supporting GET resume):

```tsx
// app/chat/[sessionId]/page.tsx
"use client";
import { useMemo, useState } from "react";
import { useChat } from "@ai-sdk/react";
import { DefaultChatTransport } from "ai";

export default function ChatPage({ params }: { params: { sessionId: string } }) {
  const [input, setInput] = useState("");
  const transport = useMemo(() => (
    new DefaultChatTransport({
      api: `/api/chat/${params.sessionId}`,
      headers: { "Content-Type": "application/json" },
      prepareSendMessagesRequest: ({ api, headers, messages, body }) => ({
        api,
        headers,
        body: {
          // send only the newest user message; server loads history from memory
          messages: messages.length ? [messages[messages.length - 1]] : [],
          ...body,
        },
      }),
      prepareReconnectToStreamRequest: ({ api, headers }) => ({ api, headers }),
    })
  ), [params.sessionId]);

  const { messages, sendMessage, status } = useChat({
    id: `chat-${params.sessionId}`,
    transport,
    resume: true,
  });

  async function onSubmit(e: React.FormEvent) {
    e.preventDefault();
    if (!input.trim() || status === "streaming") return;
    await sendMessage(input);
    setInput("");
  }

  return (
    <form onSubmit={onSubmit} className="p-4 space-y-4">
      <div className="space-y-2">
        {messages.map(m => (
          <div key={m.id}><strong>{m.role}:</strong> {m.content}</div>
        ))}
        {status === "streaming" && <div>Typing…</div>}
      </div>
      <input
        value={input}
        onChange={(e) => setInput(e.target.value)}
        className="border p-2 w-full"
        disabled={status === "streaming"}
      />
    </form>
  );
}
```

## 4) Test It

Send a message body compatible with the AI SDK shape:

```bash
curl -X POST http://localhost:3000/api/chat/demo \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "id": "1", "role": "user", "content": "Use the webSearch tool to find the latest on TypeScript 5.x features" }
    ]
  }'
```

Test GET resume (returns 204 when nothing is in flight):

```bash
curl -i http://localhost:3000/api/chat/demo
# 204 No Content = no active stream to resume
```

You should receive a streaming response. Switch to Redis to persist history:

```ts
// swap in-memory with Redis for persistence
import { RedisMemory } from "lightfast/memory/adapters/redis";
const memory = new RedisMemory({
  url: process.env.KV_REST_API_URL!,
  token: process.env.KV_REST_API_TOKEN!,
});
```

When To Switch To Redis
- Production or multiple instances
- Need resumable streams across restarts
- Shared history beyond a single server process

### Message Format Details

`fetchRequestHandler` expects an AI SDK `messages` array. Minimal shape:

```json
[
  { "id": "1", "role": "user", "content": "Hello" }
]
```

Tips:
- Only the last user message is used as the new input on POST; prior messages are persisted in memory.
- You don’t need to include prior assistant messages; the handler loads them from memory.
- For GET resume: no body is required. The handler looks up the last active stream for the session.

### Auth, Sessions, and Memory

- Always derive `resourceId` from your auth provider server‑side (e.g., Clerk, Auth.js).
- Use `InMemoryMemory` for local dev only; it resets on process restart and is not shareable across instances.
- Use `RedisMemory` for production: enables resumable streams across instances and persistent history.

### Environment Variables

- AI Gateway key: `AI_GATEWAY_API_KEY` (or provider‑specific keys per your gateway)
- Redis (Upstash): `KV_REST_API_URL`, `KV_REST_API_TOKEN` (for `RedisMemory`)

Example `.env.local`:

```bash
AI_GATEWAY_API_KEY=your-gateway-key
# Example model used above assumes Anthropic is enabled via your gateway
# Otherwise choose a model you’ve enabled, e.g. openai/gpt-4o-mini
KV_REST_API_URL=https://your-upstash-url.upstash.io
KV_REST_API_TOKEN=your-upstash-token
```

### Common Pitfalls

- 400 “no messages”: Your POST body must include `messages` with at least one user message.
- 405 “method not allowed”: Only `GET` and `POST` are supported.
- 204 on GET: No active stream to resume; this is expected when nothing is in flight.
- CORS: For Next.js App Router, call from same origin. If calling cross‑origin, configure CORS on your server.

## What’s Next

- Core concepts: `core-concepts/agents`
- Tool patterns: `agent-development/tool-factories`
- Handlers and streaming: `core-concepts/handlers`
- Memory adapters: `memory-state/memory-adapters`
