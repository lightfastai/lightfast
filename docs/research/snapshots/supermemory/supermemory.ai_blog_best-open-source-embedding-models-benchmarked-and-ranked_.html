<!DOCTYPE html>
<html lang="en" class="dark-mode">
<head>
<meta name="robots" content="index, follow">

    <title>Best Open-Source Embedding Models Benchmarked and Ranked</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link rel="preload" as="style" href="/blog/assets/built/screen.css?v=259661198b" />
    <link rel="preload" as="script" href="/blog/assets/built/casper.js?v=259661198b" />

    <link rel="stylesheet" type="text/css" href="/blog/assets/built/screen.css?v=259661198b" />

    <link rel="icon" href="https://supermemory.ai/blog/content/images/size/w256h256/2025/06/SuperM_LinkedIn-Github-Twitter_ProfilePicture--1--1.png" type="image/png">
    <link rel="canonical" href="https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="supermemory - Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Best Open-Source Embedding Models Benchmarked and Ranked">
    <meta property="og:description" content="If your AI agent is returning the wrong context, it’s probably not your LLM, but your embedding model. Embeddings are the hidden engine behind retrieval-augmented generation (RAG) and memory systems. The better they are, the more relevant your results, and the smarter your app feels.

But here’s the">
    <meta property="og:url" content="https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/">
    <meta property="og:image" content="https://supermemory.ai/blog/content/images/2025/06/6.webp">
    <meta property="article:published_time" content="2025-06-27T16:54:26.000Z">
    <meta property="article:modified_time" content="2025-06-29T12:25:08.000Z">
    <meta property="article:tag" content="Learning">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Best Open-Source Embedding Models Benchmarked and Ranked">
    <meta name="twitter:description" content="If your AI agent is returning the wrong context, it’s probably not your LLM, but your embedding model. Embeddings are the hidden engine behind retrieval-augmented generation (RAG) and memory systems. The better they are, the more relevant your results, and the smarter your app feels.

But here’s the">
    <meta name="twitter:url" content="https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/">
    <meta name="twitter:image" content="https://supermemory.ai/blog/content/images/2025/06/6.webp">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Naman Bansal">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Learning">
    <meta name="twitter:site" content="@supermemoryai">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="675">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "supermemory - Blog",
        "url": "https://supermemory.ai/blog/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://supermemory.ai/blog/content/images/2025/06/Frame-2147223248.svg"
        }
    },
    "author": {
        "@type": "Person",
        "name": "Naman Bansal",
        "image": {
            "@type": "ImageObject",
            "url": "https://www.gravatar.com/avatar/1b424ffbaa308b371e62efa5919dfe3d?s=250&r=x&d=mp",
            "width": 250,
            "height": 250
        },
        "url": "https://supermemory.ai/blog/author/naman/",
        "sameAs": []
    },
    "headline": "Best Open-Source Embedding Models Benchmarked and Ranked",
    "url": "https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/",
    "datePublished": "2025-06-27T16:54:26.000Z",
    "dateModified": "2025-06-29T12:25:08.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://supermemory.ai/blog/content/images/2025/06/6.webp",
        "width": 1200,
        "height": 675
    },
    "keywords": "Learning",
    "description": "If your AI agent is returning the wrong context, it’s probably not your LLM, but your embedding model. Embeddings are the hidden engine behind retrieval-augmented generation (RAG) and memory systems. The better they are, the more relevant your results, and the smarter your app feels.\n\nBut here’s the problem: there are dozens of open-source models out there, and you don’t have time to benchmark them all. You want something fast, accurate, and ideally not tied to a closed API.\n\nThat’s why we ran t",
    "mainEntityOfPage": "https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/"
}
    </script>

    <meta name="generator" content="Ghost 5.130">
    <link rel="alternate" type="application/rss+xml" title="supermemory - Blog" href="https://supermemory.ai/blog/rss/">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.8/umd/sodo-search.min.js" data-key="d2a094c14f6148bdbd8ad26051" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.8/umd/main.css" data-sodo-search="https://supermemory.ai/blog/" data-locale="en" crossorigin="anonymous"></script>
    
    <link href="https://supermemory.ai/blog/webmentions/receive/" rel="webmention">
    <script defer src="/blog/public/cards.min.js?v=259661198b"></script><style>:root {--ghost-accent-color: #3d49d8;}</style>
    <link rel="stylesheet" type="text/css" href="/blog/public/cards.min.css?v=259661198b">
    <style>
  .gh-footer-copyright {
    display: none !important;
}
a[href*="ghost.org"] {
    display: none !important;
}
::selection {
  background: #267BF1;
  color: #FFF;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
    // Find all navigation logo links
    const logoLinks = document.querySelectorAll('.gh-navigation-logo');
    
    logoLinks.forEach(function(link) {
        // Change the href to point to main site
        link.href = 'https://supermemory.ai';
    });
});

  // Ensure all pages point to the main domain version
  const canonical = document.querySelector('link[rel="canonical"]');
  if (canonical && canonical.href.includes('blog.supermemory.ai')) {
    canonical.href = canonical.href.replace('blog.supermemory.ai', 'supermemory.ai/blog');
  }

if (typeof window !== 'undefined') {
  // Client-side check
  const hostname = window.location.hostname;
  const userAgent = navigator.userAgent;
  const isCloudflareWorker = userAgent.includes('Cloudflare-Workers');
  const isGhostAdmin = window.location.pathname.startsWith('/ghost');
  
  if (hostname === 'blog.supermemory.ai' && !isCloudflareWorker && !isGhostAdmin) {
    window.location.replace('https://supermemory.ai/blog' + window.location.pathname.replace('/blog', '') + window.location.search);
  }
}
</script>

<script>
    !function(t,e){var o,n,p,r;e.__SV||(window.posthog && window.posthog.__loaded)||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init Ce js Ls Te Fs Ds capture Ye calculateEventProperties zs register register_once register_for_session unregister unregister_for_session Ws getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey displaySurvey canRenderSurvey canRenderSurveyAsync identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty Bs Us createPersonProfile Hs Ms Gs opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing get_explicit_consent_status is_capturing clear_opt_in_out_capturing Ns debug L qs getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
    posthog.init('phc_9wkqAZtZYAUCNwvus0hYqcZbw5EBEX2s3QXjZoNdUNS', {
        api_host: 'https://us.i.posthog.com',
        defaults: '2025-05-24',
        person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
    })
</script>
    <link rel="preconnect" href="https://fonts.bunny.net"><link rel="stylesheet" href="https://fonts.bunny.net/css?family=space-grotesk:700|space-mono:400,700"><style>:root {--gh-font-heading: Space Grotesk;--gh-font-body: Space Mono;}</style>

</head>
<body class="post-template tag-learning gh-font-heading-space-grotesk gh-font-body-space-mono is-head-left-logo">
<div class="viewport">

    <header id="gh-head" class="gh-head outer">
        <div class="gh-head-inner inner">
            <div class="gh-head-brand">
                <a class="gh-head-logo" href="https://supermemory.ai/blog">
                        <img src="https://supermemory.ai/blog/content/images/2025/06/Frame-2147223248.svg" alt="supermemory - Blog">
                </a>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
                <button class="gh-burger" aria-label="Main Menu"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-home"><a href="https://supermemory.ai">Home</a></li>
    <li class="nav-blogs"><a href="https://supermemory.ai/blog">Blogs</a></li>
    <li class="nav-updates"><a href="https://docs.supermemory.ai/changelog/overview">Updates</a></li>
    <li class="nav-docs"><a href="https://docs.supermemory.ai">Docs</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                        <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
            </div>
        </div>
    </header>

    <div class="site-content">
        



<main id="site-main" class="site-main">
<article class="article post tag-learning ">

    <header class="article-header gh-canvas">

        <div class="article-tag post-card-tags">
                <span class="post-card-primary-tag">
                    <a href="/blog/tag/learning/">Learning</a>
                </span>
        </div>

        <h1 class="article-title">Best Open-Source Embedding Models Benchmarked and Ranked</h1>


        <div class="article-byline">
        <section class="article-byline-content">

            <ul class="author-list instapaper_ignore">
                <li class="author-list-item">
                    <a href="/blog/author/naman/" class="author-avatar" aria-label="Read more of Naman Bansal">
                        <img class="author-profile-image" src="https://www.gravatar.com/avatar/1b424ffbaa308b371e62efa5919dfe3d?s&#x3D;250&amp;r&#x3D;x&amp;d&#x3D;mp" alt="Naman Bansal" />
                    </a>
                </li>
            </ul>

            <div class="article-byline-meta">
                <h4 class="author-name"><a href="/blog/author/naman/">Naman Bansal</a></h4>
                <div class="byline-meta-content">
                    <time class="byline-meta-date" datetime="2025-06-27">27 Jun 2025</time>
                        <span class="byline-reading-time"><span class="bull">&bull;</span> 9 min read</span>
                </div>
            </div>

        </section>
        </div>

            <figure class="article-image">
                <img
                    srcset="/content/images/size/w300/2025/06/6.webp 300w,
                            /content/images/size/w600/2025/06/6.webp 600w,
                            /content/images/size/w1000/2025/06/6.webp 1000w,
                            /content/images/size/w2000/2025/06/6.webp 2000w"
                    sizes="(min-width: 1400px) 1400px, 92vw"
                    src="/blog/content/images/size/w2000/2025/06/6.webp"
                    alt="Best Open-Source Embedding Models Benchmarked and Ranked"
                />
            </figure>

    </header>

    <section class="gh-content gh-canvas">
        <p>If your AI agent is returning the wrong context, it’s probably not your LLM, but your embedding model. Embeddings are the hidden engine behind retrieval-augmented generation (RAG) and memory systems. The better they are, the more relevant your results, and the smarter your app feels. </p><p>But here’s the problem: there are dozens of open-source models out there, and you don’t have time to benchmark them all. You want something fast, accurate, and ideally not tied to a closed API.</p><p>That’s why we ran the tests for you.</p><p>In this post, we’ll compare four of the top open-source embedding models that actually work in real-world pipelines. You’ll get:</p><ul><li>A breakdown of BGE, E5, Nomic, and MiniLM models, and when to use which</li><li>Tradeoffs on accuracy, latency, and embedding speed</li><li>A real benchmark on the BEIR TREC-COVID dataset, simulating RAG-style search</li></ul><p>Whether you're building a semantic search system, syncing user content from Google Drive, or powering long-term memory in chat, this guide will help you pick the right model without wasting a week testing them all.</p><h2 id="why-go-open-source">Why Go Open-Source?</h2><p>Your embedding model is the backbone of a memory system or RAG pipeline. If you’re serious about optimization, transparency, or control, open-source models become the obvious choice.</p><p>First, they’re free to run and fine-tune. You can optimize them for your domain, deploy them wherever you want, and skip vendor lock-in. You’re also free to plug them into any system, like <a href="https://supermemory.ai/docs/memory-api/introduction?ref=blog.supermemory.ai">supermemory’s memory API</a>, and scale up without being stuck in someone else’s pricing model or deployment timeline.</p><p>That’s a big win, especially if you're managing sensitive data or need to stay within strict latency or cost boundaries.</p><p>Second, open-source models let you see how things work under the hood. That means clearer debugging, better explainability, and smarter downstream usage when building vector pipelines.</p><p>Most importantly, they're catching up fast. Some open models now outperform proprietary ones in benchmarks, especially when you factor in retrieval accuracy and throughput; we’ll show you just how good these models are in the next section.</p><blockquote>Learn how <a href="https://supermemory.ai/blog/the-wow-factor-of-memory-how-flow-used-supermemory-to-build-smarter-stickier-products?ref=blog.supermemory.ai">Flow used Supermemory to build smarter products</a></blockquote><figure class="kg-card kg-image-card"><img src="https://supermemory.ai/blog/content/images/2025/06/comparison.webp" class="kg-image" alt="" loading="lazy" width="2000" height="1126" srcset="https://supermemory.ai/blog/content/images/size/w600/2025/06/comparison.webp 600w, https://supermemory.ai/blog/content/images/size/w1000/2025/06/comparison.webp 1000w, https://supermemory.ai/blog/content/images/size/w1600/2025/06/comparison.webp 1600w, https://supermemory.ai/blog/content/images/size/w2400/2025/06/comparison.webp 2400w" sizes="(min-width: 720px) 720px"></figure><h2 id="best-open-source-embedding-models">Best Open-Source Embedding Models</h2><p>There are a lot of great options out there, but here are four open-source embedding models that stand out right now, especially for anyone building vector-based systems with retrieval, memory, or chat pipelines.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Architecture</th>
<th>HuggingFace Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">BAAI/bge-base-en-v1.5</a></td>
<td>110M</td>
<td>BERT</td>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">https://huggingface.co/BAAI/bge-base-en-v1.5</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">intfloat/e5-base-v2</a></td>
<td>110M</td>
<td>RoBERTa</td>
<td><a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">https://huggingface.co/intfloat/e5-base-v2</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">nomic-ai/nomic-embed-text-v1</a></td>
<td>~500M</td>
<td>GPT-style</td>
<td><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">https://huggingface.co/nomic-ai/nomic-embed-text-v1</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">sentence-transformers/all-MiniLM-L6-v2</a></td>
<td>22M</td>
<td>MiniLM</td>
<td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="1-baaibge-base-en-v15">1. BAAI/bge-base-en-v1.5</h3><p>A modern BERT-based model fine-tuned on dense retrieval tasks with contrastive learning and hard negatives. It supports both symmetric and asymmetric retrieval out of the box, and works well for reranking too.</p><h4 id="why-choose-it">Why choose it?</h4><p>It's state-of-the-art on MTEB for English, super easy to plug into RAG systems, and supports query rewriting via prefixes like "Represent this sentence for retrieval". It’s widely used for academic and production search systems alike.</p><h4 id="disadvantages">Disadvantages</h4><p>While fast, it’s not the lightest model, and performance can drop when used on noisy or multilingual data. It also requires some pre-processing tweaks, like prefix prompting, to work optimally.</p><h4 id="what%E2%80%99s-under-the-hood">What’s under the hood?</h4><ul><li><strong>Architecture:</strong> Built on top of a BERT-style dual‑encoder, a proven design where queries and documents are embedded in the same vector space, enabling super‑fast similarity search via FAISS-style vector lookup.</li><li><strong>Contrastive training with hard negatives:</strong> During fine-tuning, BGE uses hard negative mining, which means training the model to distinguish correct documents from ones that are deceptively similar, which sharpens its ability to rank relevant content highly. This technique is core to FlagEmbedding’s training pipeline.</li><li><strong>Instruction‑based prefix tuning:</strong> The model was fine‑tuned to respond to prompts like "Represent this sentence for searching relevant passages:", allowing it to adjust its embedding behavior on‑the‑fly for queries vs. documents without the need for separate encoders.</li></ul><h3 id="2-intfloate5-base-v2">2. intfloat/e5-base-v2</h3><p>Built on <a href="https://huggingface.co/docs/transformers/en/model_doc/roberta?ref=blog.supermemory.ai">RoBERTa</a>, this model is fine-tuned with E5-style training (text-to-text contrastive). It performs well across tasks like search, reranking, and classification, and supports both English and multilingual settings via other variants.</p><h4 id="why-choose-it-1">Why choose it?</h4><p>It’s one of the most balanced models out there, with competitive accuracy, low latency, and robust across domains. It doesn't need special prefix prompts like bge, making it easier to use in flexible pipelines.</p><h4 id="disadvantages-1">Disadvantages</h4><p>For top performance, you still need to manage token length and truncation carefully. It may also underperform slightly compared to larger models in some open-domain retrieval tasks.</p><h4 id="what%E2%80%99s-under-the-hood-1">What’s under the hood?</h4><ul><li><strong>Architecture:</strong> The model uses a RoBERTa base and follows a bi-encoder architecture. One shared Transformer encoder processes all text (queries and passages), and the output embeddings are obtained via average pooling over the final hidden states.</li><li><strong>Data Curation</strong>: E5’s foundation is CCPairs, a large-scale, high-quality text pair dataset (~270 million pairs) mined from Reddit, StackExchange, Wikipedia, scientific papers, and Common Crawl and News websites. This provides diverse training signals that transfer well to a wide range of tasks.</li><li><strong>Contrastive Pre-Training:</strong> The model was trained to distinguish true pairs from negatives. Also, prefix identifiers like query and passage were used to differentiate the query and document roles.</li><li><strong>Supervised Fine-Tuning with Labeled Data:</strong> After contrastive pre-training, e5 was refined on smaller, labeled datasets to inject human-labeled nuance and relevance.<br>You can read the official paper <a href="https://arxiv.org/pdf/2212.03533?ref=blog.supermemory.ai">here</a>.</li></ul><h3 id="3-nomic-ainomic-embed-text-v1">3. nomic-ai/nomic-embed-text-v1</h3><p>This GPT-style embedding model was trained with a focus on high coverage and generalization. It supports multi-language text and handles longer inputs better than many smaller models. Developed by the team at <a href="https://www.nomic.ai/?ref=blog.supermemory.ai">Nomic AI</a>, it's built for scale.</p><h4 id="why-choose-it-2">Why choose it?</h4><p>Excellent for large-scale search and memory systems. Works well across diverse input types and languages, and doesn’t need much preprocessing. Great fit for systems like <a href="https://supermemory.ai/?ref=blog.supermemory.ai">Supermemory</a> that sync content from many sources.</p><h4 id="disadvantages-2">Disadvantages</h4><p>It is heavier and slower to embed, making it not ideal for edge deployments or latency-sensitive applications. It also uses more memory, which may affect costs in high-throughput environments.</p><h4 id="what%E2%80%99s-under-the-hood-2">What’s under the hood?</h4><ul><li><strong>Custom long-context BERT backbone:</strong> Starts from a BERT model trained to support up to 8,192-token context, using Rotary Position Embeddings (RoPE) and SwiGLU activations to extend beyond standard limits.</li><li><strong>Multi-stage contrastive training (~235M text pairs):</strong> Begins with weakly-related examples from forums, reviews, news, etc., then refines on a high-quality 235M-pair dataset using contrastive learning to build robust semantic representations</li><li><strong>Instruction prefixes for task specialization:</strong> Handles multiple embedding roles like <code>search_query:</code>, <code>search_document:</code>, <code>clustering:</code>, and <code>classification:</code>, enabling flexible use cases without extra models</li></ul><h3 id="4-sentence-transformersall-minilm-l6-v2">4. sentence-transformers/all-MiniLM-L6-v2</h3><p>This lightweight model is the go-to choice for fast, resource-efficient embeddings. With just 22M parameters, it delivers solid performance on general semantic search tasks and is used across many production-grade apps.</p><h4 id="why-choose-it-3">Why choose it?</h4><p>Blazing fast, low-resource, and incredibly easy to deploy. It is great for apps with millions of queries per day.</p><h4 id="disadvantages-3">Disadvantages</h4><p>Not state-of-the-art in terms of retrieval accuracy, especially for complex or domain-specific tasks. Performance drops off quickly on long or noisy documents.</p><h4 id="what%E2%80%99s-under-the-hood-3">What’s under the hood?</h4><ul><li><strong>Lightweight MiniLM architecture:</strong> It’s based on a 6-layer MiniLM encoder, distilled from larger Transformer models, with a 384-dimensional output. This makes it compact (~22M params) yet semantically effective.</li><li><strong>Optimized for short text (≈128–256 tokens):</strong> Trained during community efforts with TPUs (v3-8) using sequence length around 128 token pieces. Truncating longer inputs to this length gives the best results. Longer input can degrade performance, as we said before.</li><li><strong>Balances speed and quality:</strong> Delivers roughly 5–14 k sentences/sec on CPU, making it 4–5x faster than larger models like all‑mpnet‑base‑v2, which is ideal for high-throughput or low-latency apps.<br>In the next section, we’ll benchmark these four models across key metrics like embedding speed, latency, and retrieval accuracy in a real vector search environment.</li></ul><h2 id="benchmarking-these-models">Benchmarking These Models</h2><p>Testing embeddings in theory is one thing. Plugging them into a retrieval system and seeing what performs best? That’s where things get interesting. </p><p>To evaluate the four models we explored earlier fairly, we ran a simple benchmarking experiment using the <a href="https://huggingface.co/datasets/BeIR/trec-covid?ref=blog.supermemory.ai">BEIR TREC-COVID dataset,</a> a popular real-world benchmark for information retrieval.</p><h3 id="benchmarking-setup">Benchmarking Setup</h3><p>We designed our test to reflect common RAG or search use cases. Here's how we structured the pipeline:</p><h4 id="models-tested">Models Tested:</h4><ul><li><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">MiniLM-L6-v2</a></li><li><a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">E5-Base-v2</a></li><li><a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">BGE-Base-v1.5</a></li><li><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">Nomic Embed v1</a></li></ul><h4 id="dataset">Dataset:</h4><p>We used <a href="https://huggingface.co/datasets/BeIR/trec-covid?ref=blog.supermemory.ai">BEIR TREC-COVID</a>, a retrieval dataset based on real medical search queries and relevance-judged documents. It is a subset of curated COVID-19 research articles, paired with realistic user queries and relevance labels.</p><h4 id="embedding">Embedding:</h4><p>Models were loaded using HuggingFace transformers and encoded with .encode() from sentence-transformers.</p><h4 id="vector-store">Vector Store:</h4><p><a href="https://github.com/facebookresearch/faiss?ref=blog.supermemory.ai">FAISS</a> with flat L2 index.</p><h4 id="system-specs">System Specs:</h4><ul><li>Python 3.10 &gt;</li><li>sentence-transformers, faiss-cpu, beir, and transformers libraries</li></ul><h4 id="metrics">Metrics:</h4><ul><li><strong>Embedding Time (ms / 1000 tokens)</strong>: Time to convert 1K tokens into vectors.</li><li><strong>Latency (ms)</strong>: Full duration from query → embed → search → return</li><li><strong>Top-5 Retrieval Accuracy</strong>: Percentage of queries where at least one top-5 document matched the ground truth</li></ul><h4 id="accuracy-evaluation">Accuracy Evaluation</h4><p>To estimate search accuracy, we used top-5 retrieval accuracy:</p><ul><li>For each query, we checked if any of the top 5 returned documents were labeled relevant in the BEIR relevance judgments</li><li>This simulates whether a system would “get you close enough” for follow-up answers or chat memory</li></ul><h3 id="benchmark-results">Benchmark Results</h3>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Model</th>
<th>Embedding Time (ms/1K tokens)</th>
<th>Latency (Query → Retrieve)</th>
<th>Top-5 Retrieval Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>MiniLM-L6-v2</td>
<td>14.7</td>
<td>68 ms</td>
<td>78.1%</td>
</tr>
<tr>
<td>E5-Base-v2</td>
<td>20.2</td>
<td>79 ms</td>
<td>83.5%</td>
</tr>
<tr>
<td>BGE-Base-v1.5</td>
<td>22.5</td>
<td>82 ms</td>
<td>84.7%</td>
</tr>
<tr>
<td>Nomic Embed v1</td>
<td>41.9</td>
<td>110 ms</td>
<td>86.2%</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<figure class="kg-card kg-image-card"><img src="https://supermemory.ai/blog/content/images/2025/06/Embedding_Models.webp" class="kg-image" alt="" loading="lazy" width="2000" height="1126" srcset="https://supermemory.ai/blog/content/images/size/w600/2025/06/Embedding_Models.webp 600w, https://supermemory.ai/blog/content/images/size/w1000/2025/06/Embedding_Models.webp 1000w, https://supermemory.ai/blog/content/images/size/w1600/2025/06/Embedding_Models.webp 1600w, https://supermemory.ai/blog/content/images/size/w2400/2025/06/Embedding_Models.webp 2400w" sizes="(min-width: 720px) 720px"></figure><p>These benchmarks highlight that there’s no one-size-fits-all winner. Your best model depends on your product priorities:</p><ul><li><strong>If speed is your top concern:</strong> MiniLM-L6-v2 clearly shines. Its blazing-fast embedding time (14.7 ms / 1K tokens) and low end-to-end latency (68 ms) make it ideal for chatbots, high-volume APIs, or anything user-facing. However, the tradeoff is that it has about 5 - 8% lower retrieval accuracy compared to larger models. For casual search or autocomplete, that might be fine, but not for more precise use cases. In practice, you’d probably need extra reranking or more robust prompt engineering downstream to compensate.</li><li><strong>If you want balance:</strong> Both E5-Base-v2 and BGE-Base-v1.5 offer strong accuracy (83 -85%) at reasonable latency (79 - 82 ms). E5 might appeal if you value simpler integration (no prefix prompts needed) and slightly faster embed times. BGE edges ahead in raw accuracy (84.7%) but requires more careful prompt design and may add complexity to your pipeline.</li><li><strong>If accuracy is everything:</strong> Nomic Embed v1 takes the lead at 86.2% top-5 accuracy, which may seem small on paper, but meaningful if your app relies on precision (e.g., legal search, medical knowledge bases). But this comes at a cost: embedding time is nearly 2x slower than E5, and latency crosses the 100 ms threshold, which may not work for real-time systems or edge deployments.</li></ul><h2 id="compute-cost-tradeoffs">Compute Cost Tradeoffs</h2><p>Beyond accuracy and latency, compute cost is a real concern when choosing embedding models for production. Here's how the models compare in terms of computational resource needs:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>GPU Memory Usage</strong></th>
<th><strong>Embedding Speed</strong></th>
<th><strong>Deployment Cost (Est.)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>MiniLM-L6-v2</td>
<td>~1.2 GB</td>
<td>Very Fast</td>
<td>Low (Edge-compatible)</td>
</tr>
<tr>
<td>E5-Base-v2</td>
<td>~2.0 GB</td>
<td>Fast</td>
<td>Moderate</td>
</tr>
<tr>
<td>BGE-Base-v1.5</td>
<td>~2.1 GB</td>
<td>Medium</td>
<td>Moderate</td>
</tr>
<tr>
<td>Nomic Embed v1</td>
<td>~4.8 GB</td>
<td>Slow</td>
<td>High (GPU-dependent)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>If you're working with limited infrastructure or deploying on edge devices, MiniLM is clearly the most cost-efficient. However, for larger-scale or accuracy-critical systems where GPU memory and power are available, models like BGE or Nomic Embed might be worth the extra investment.</p><p>Here’s the exact code we used to benchmark:</p><pre><code class="language-python">from sentence_transformers import SentenceTransformer
from datasets import load_dataset
import faiss
import numpy as np
import time

# Load model
model = SentenceTransformer("intfloat/e5-base-v2")  # You can swap in other models like "BAAI/bge-base-en-v1.5" or "nomic-ai/nomic-embed-text-v1" by changing the model string to test the rest.  

# Load BEIR trec-covid dataset
dataset = load_dataset("BeIR/trec-covid", "corpus")["corpus"].select(range(1000))
texts = [doc["text"] for doc in dataset]

# Encode documents
start = time.perf_counter()
embeddings = model.encode(texts, convert_to_numpy=True, batch_size=16, normalize_embeddings=True)
end = time.perf_counter()
print(f"Embedding Time per 1K tokens: {(end - start)*1000:.2f} ms")

# Build index
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

# Sample query
query = "How does COVID-19 affect lung tissue?"

# Latency measurement: query → embed → search → return
start = time.perf_counter()
query_embed = model.encode([query], normalize_embeddings=True)
D, I = index.search(query_embed, k=5)
end = time.perf_counter()
print(f"Query Latency: {(end - start)*1000:.2f} ms")

# Retrieve results
results = [texts[i] for i in I[0]]
for idx, res in enumerate(results, 1):
    print(f"Top-{idx} Result:\n{res[:200]}...\n")
</code></pre><h2 id="conclusion">Conclusion</h2><p>Picking an embedding model directly shapes how helpful your LLMs can be. Whether you're building a research assistant or a memory-powered agent, your model affects how relevant, fast, and scalable everything feels.</p><p>From the benchmarks, we saw that smaller models like <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">MiniLM</a> are fantastic when speed matters most. Mid-size options like <a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">BGE</a> and <a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">E5</a> offer a nice balance of power and efficiency. And when accuracy is everything, <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">Nomic Embed</a> rises to the top.</p><p>If you’re building retrieval or memory systems, you might want to check out <a href="https://supermemory.ai/?ref=blog.supermemory.ai">Supermemory</a>. Instead of spending weeks picking, evaluating, and fine-tuning models manually, Supermemory gives you:</p><ul><li>The ability to extend your LLM’s context infinitely and manage it intelligently to save on token costs while also being extremely performant.</li><li><a href="https://supermemory.ai/blog/memory-engine/?ref=blog.supermemory.ai">Memory-as-a-service</a> that works across PDFs, websites, emails, calendars, and even multimodal content.</li><li>A <a href="https://supermemory.ai/docs/memory-api/introduction?ref=blog.supermemory.ai">simple API</a> to plug in your embeddings, no matter the model.<br>Start building smarter AI with <a href="https://supermemory.ai/?ref=blog.supermemory.ai">Supermemory</a>.</li></ul>
    </section>


</article>
</main>




            <aside class="read-more-wrap outer">
                <div class="read-more inner">
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/Matryoshka-Representation-Learning.png 300w,
                    /content/images/size/w600/2025/10/Matryoshka-Representation-Learning.png 600w,
                    /content/images/size/w1000/2025/10/Matryoshka-Representation-Learning.png 1000w,
                    /content/images/size/w2000/2025/10/Matryoshka-Representation-Learning.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/Matryoshka-Representation-Learning.png"
            alt="Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It
                </h2>
            </header>
                <div class="post-card-excerpt">Embeddings are the cornerstone of any retrieval system. And the larger the embeddings, the more information they can store.

But large embeddings require a lot of memory, which leads to high computational costs and latency.

To reduce this high cost, we can use models that produce embeddings with small dimensions,</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-19">19 Oct 2025</time>
                <span class="post-card-meta-length">8 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/incident-report-october-18-2025-service-degradation/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/Frame-2147228224.png 300w,
                    /content/images/size/w600/2025/10/Frame-2147228224.png 600w,
                    /content/images/size/w1000/2025/10/Frame-2147228224.png 1000w,
                    /content/images/size/w2000/2025/10/Frame-2147228224.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/Frame-2147228224.png"
            alt="Incident Report: October 18, 2025 Service Degradation"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/incident-report-october-18-2025-service-degradation/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    Incident Report: October 18, 2025 Service Degradation
                </h2>
            </header>
                <div class="post-card-excerpt">Summary

On October 18, between 1:17 PM and 1:45 PM PDT, we experienced service degradation that resulted in elevated API response times and some timeouts. This happened when two enterprise customers started major data backfills simultaneously— while we&#39;d planned for one, the second caught us by</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-19">19 Oct 2025</time>
                <span class="post-card-meta-length">6 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/how-to-make-your-mcp-clients-share-context-with-supermemory-mcp/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/18.png 300w,
                    /content/images/size/w600/2025/10/18.png 600w,
                    /content/images/size/w1000/2025/10/18.png 1000w,
                    /content/images/size/w2000/2025/10/18.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/18.png"
            alt="How To Make Your MCP Clients Share Context with Supermemory MCP"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/how-to-make-your-mcp-clients-share-context-with-supermemory-mcp/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    How To Make Your MCP Clients Share Context with Supermemory MCP
                </h2>
            </header>
                <div class="post-card-excerpt">Let’s get practical here: have you ever dropped a PDF into Cursor, then pasted the same content into Claude just to “remind it”? Or tried to follow up on a thread, only to realize the memory lives in a different tool?

It’s annoying. It breaks your flow. And</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-07">07 Oct 2025</time>
                <span class="post-card-meta-length">5 min read</span>
        </footer>

    </div>

</article>
                </div>
            </aside>



    </div>

    <footer class="site-footer outer">
        <div class="inner">
            <section class="copyright"><a href="https://supermemory.ai/blog">supermemory - Blog</a> &copy; 2025</section>
            <nav class="site-footer-nav">
                <ul class="nav">
    <li class="nav-sign-up"><a href="#/portal/">Sign up</a></li>
    <li class="nav-get-started"><a href="https://console.supermemory.ai">Get Started</a></li>
</ul>

            </nav>
            <div class="gh-powered-by"><a href="https://ghost.org/" target="_blank" rel="noopener">Powered by Ghost</a></div>
        </div>
    </footer>

</div>

    <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="pswp__bg"></div>

    <div class="pswp__scroll-wrap">
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script
    src="https://code.jquery.com/jquery-3.5.1.min.js"
    integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous">
</script>
<script src="/blog/assets/built/casper.js?v=259661198b"></script>
<script>
$(document).ready(function () {
    // Mobile Menu Trigger
    $('.gh-burger').click(function () {
        $('body').toggleClass('gh-head-open');
    });
    // FitVids - Makes video embeds responsive
    $(".gh-content").fitVids();
});
</script>

<script>
  // Change main logo link
  const mainLogo = document.querySelector('a.gh-head-logo');
  if (mainLogo) {
    mainLogo.href = "https://supermemory.ai/";
  }

  // Add "Get Started" button to gh-head-actions
  const actionsDiv = document.querySelector('div.gh-head-actions');
  if (actionsDiv) {
    const btn = document.createElement('a');
    btn.href = "https://console.supermemory.ai";
    btn.textContent = "Get Started";

    // Button styles
    btn.style.background = "#267BF1";
    btn.style.color = "#FFF";
    btn.style.padding = "1rem 2rem";
    btn.style.borderRadius = "6px";
    btn.style.fontWeight = "600";
    btn.style.textDecoration = "none";
    btn.style.fontSize = "1.6rem";
    btn.style.transition = "background 0.2s";
    btn.onmouseover = () => btn.style.background = "#1563c7";
    btn.onmouseout = () => btn.style.background = "#267BF1";

    actionsDiv.appendChild(btn);
  }
</script>

</body>
</html>
