<!DOCTYPE html>
<html lang="en" class="dark-mode">
<head>
<meta name="robots" content="index, follow">

    <title>Architecting a memory engine inspired by the human brain</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link rel="preload" as="style" href="/blog/assets/built/screen.css?v=259661198b" />
    <link rel="preload" as="script" href="/blog/assets/built/casper.js?v=259661198b" />

    <link rel="stylesheet" type="text/css" href="/blog/assets/built/screen.css?v=259661198b" />

    <link rel="icon" href="https://supermemory.ai/blog/content/images/size/w256h256/2025/06/SuperM_LinkedIn-Github-Twitter_ProfilePicture--1--1.png" type="image/png">
    <link rel="canonical" href="https://supermemory.ai/blog/memory-engine/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="supermemory - Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Architecting a memory engine inspired by the human brain">
    <meta property="og:description" content="Language is at the heart of intelligence, but what truly powers meaningful interaction is memory ‚Äî the ability to accumulate, recall, and contextualize information over time.

Large Language Models (LLMs) have mastered language, but memory remains their Achilles‚Äô heel. Every leap in context window size is quickly outpaced by real-world demands:">
    <meta property="og:url" content="https://supermemory.ai/blog/memory-engine/">
    <meta property="og:image" content="https://supermemory.ai/blog/content/images/2025/06/blog-cover.png">
    <meta property="article:published_time" content="2025-06-05T19:20:31.000Z">
    <meta property="article:modified_time" content="2025-06-05T19:22:20.000Z">
    <meta property="article:tag" content="Engineering">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Architecting a memory engine inspired by the human brain">
    <meta name="twitter:description" content="Language is at the heart of intelligence, but what truly powers meaningful interaction is memory ‚Äî the ability to accumulate, recall, and contextualize information over time.

Large Language Models (LLMs) have mastered language, but memory remains their Achilles‚Äô heel. Every leap in context window size is quickly outpaced by real-world demands:">
    <meta name="twitter:url" content="https://supermemory.ai/blog/memory-engine/">
    <meta name="twitter:image" content="https://supermemory.ai/blog/content/images/2025/06/blog-cover.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Dhravya Shah">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Engineering">
    <meta name="twitter:site" content="@supermemoryai">
    <meta name="twitter:creator" content="@dhravyashah">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="675">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "supermemory - Blog",
        "url": "https://supermemory.ai/blog/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://supermemory.ai/blog/content/images/2025/06/Frame-2147223248.svg"
        }
    },
    "author": {
        "@type": "Person",
        "name": "Dhravya Shah",
        "image": {
            "@type": "ImageObject",
            "url": "https://supermemory.ai/blog/content/images/2025/06/90306c68c41aba2b0c763682af4907f6.png",
            "width": 128,
            "height": 128
        },
        "url": "https://supermemory.ai/blog/author/dhravya/",
        "sameAs": [
            "https://dhravya.dev",
            "https://x.com/dhravyashah"
        ]
    },
    "contributor": [
        {
            "@type": "Person",
            "name": "Soham Daga",
            "image": {
                "@type": "ImageObject",
                "url": "https://supermemory.ai/blog/content/images/2025/06/soham-gc.jpg"
            },
            "url": "https://supermemory.ai/blog/author/soham/",
            "sameAs": [
                "https://sohamdaga.com",
                "https://x.com/sohamdaga22",
                "https://www.linkedin.com/in/soham-daga"
            ]
        }
    ],
    "headline": "Architecting a memory engine inspired by the human brain",
    "url": "https://supermemory.ai/blog/memory-engine/",
    "datePublished": "2025-06-05T19:20:31.000Z",
    "dateModified": "2025-06-05T19:22:20.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://supermemory.ai/blog/content/images/2025/06/blog-cover.png",
        "width": 1200,
        "height": 675
    },
    "keywords": "Engineering",
    "description": "Language is at the heart of intelligence, but what truly powers meaningful interaction is memory ‚Äî the ability to accumulate, recall, and contextualize information over time.\n\nLarge Language Models (LLMs) have mastered language, but memory remains their Achilles‚Äô heel. Every leap in context window size is quickly outpaced by real-world demands: users upload massive documents, have lengthy conversations, and expect seamless recall of preferences and history. The result? LLMs that forget, hallucin",
    "mainEntityOfPage": "https://supermemory.ai/blog/memory-engine/"
}
    </script>

    <meta name="generator" content="Ghost 5.130">
    <link rel="alternate" type="application/rss+xml" title="supermemory - Blog" href="https://supermemory.ai/blog/rss/">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.8/umd/sodo-search.min.js" data-key="d2a094c14f6148bdbd8ad26051" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.8/umd/main.css" data-sodo-search="https://supermemory.ai/blog/" data-locale="en" crossorigin="anonymous"></script>
    
    <link href="https://supermemory.ai/blog/webmentions/receive/" rel="webmention">
    <script defer src="/blog/public/cards.min.js?v=259661198b"></script><style>:root {--ghost-accent-color: #3d49d8;}</style>
    <link rel="stylesheet" type="text/css" href="/blog/public/cards.min.css?v=259661198b">
    <style>
  .gh-footer-copyright {
    display: none !important;
}
a[href*="ghost.org"] {
    display: none !important;
}
::selection {
  background: #267BF1;
  color: #FFF;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
    // Find all navigation logo links
    const logoLinks = document.querySelectorAll('.gh-navigation-logo');
    
    logoLinks.forEach(function(link) {
        // Change the href to point to main site
        link.href = 'https://supermemory.ai';
    });
});

  // Ensure all pages point to the main domain version
  const canonical = document.querySelector('link[rel="canonical"]');
  if (canonical && canonical.href.includes('blog.supermemory.ai')) {
    canonical.href = canonical.href.replace('blog.supermemory.ai', 'supermemory.ai/blog');
  }

if (typeof window !== 'undefined') {
  // Client-side check
  const hostname = window.location.hostname;
  const userAgent = navigator.userAgent;
  const isCloudflareWorker = userAgent.includes('Cloudflare-Workers');
  const isGhostAdmin = window.location.pathname.startsWith('/ghost');
  
  if (hostname === 'blog.supermemory.ai' && !isCloudflareWorker && !isGhostAdmin) {
    window.location.replace('https://supermemory.ai/blog' + window.location.pathname.replace('/blog', '') + window.location.search);
  }
}
</script>

<script>
    !function(t,e){var o,n,p,r;e.__SV||(window.posthog && window.posthog.__loaded)||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init Ce js Ls Te Fs Ds capture Ye calculateEventProperties zs register register_once register_for_session unregister unregister_for_session Ws getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey displaySurvey canRenderSurvey canRenderSurveyAsync identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty Bs Us createPersonProfile Hs Ms Gs opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing get_explicit_consent_status is_capturing clear_opt_in_out_capturing Ns debug L qs getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
    posthog.init('phc_9wkqAZtZYAUCNwvus0hYqcZbw5EBEX2s3QXjZoNdUNS', {
        api_host: 'https://us.i.posthog.com',
        defaults: '2025-05-24',
        person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
    })
</script>
    <link rel="preconnect" href="https://fonts.bunny.net"><link rel="stylesheet" href="https://fonts.bunny.net/css?family=space-grotesk:700|space-mono:400,700"><style>:root {--gh-font-heading: Space Grotesk;--gh-font-body: Space Mono;}</style>

</head>
<body class="post-template tag-engineering gh-font-heading-space-grotesk gh-font-body-space-mono is-head-left-logo">
<div class="viewport">

    <header id="gh-head" class="gh-head outer">
        <div class="gh-head-inner inner">
            <div class="gh-head-brand">
                <a class="gh-head-logo" href="https://supermemory.ai/blog">
                        <img src="https://supermemory.ai/blog/content/images/2025/06/Frame-2147223248.svg" alt="supermemory - Blog">
                </a>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
                <button class="gh-burger" aria-label="Main Menu"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-home"><a href="https://supermemory.ai">Home</a></li>
    <li class="nav-blogs"><a href="https://supermemory.ai/blog">Blogs</a></li>
    <li class="nav-updates"><a href="https://docs.supermemory.ai/changelog/overview">Updates</a></li>
    <li class="nav-docs"><a href="https://docs.supermemory.ai">Docs</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                        <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
            </div>
        </div>
    </header>

    <div class="site-content">
        



<main id="site-main" class="site-main">
<article class="article post tag-engineering featured ">

    <header class="article-header gh-canvas">

        <div class="article-tag post-card-tags">
                <span class="post-card-primary-tag">
                    <a href="/blog/tag/engineering/">Engineering</a>
                </span>
                <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
        </div>

        <h1 class="article-title">Architecting a memory engine inspired by the human brain</h1>


        <div class="article-byline">
        <section class="article-byline-content">

            <ul class="author-list instapaper_ignore">
                <li class="author-list-item">
                    <a href="/blog/author/dhravya/" class="author-avatar" aria-label="Read more of Dhravya Shah">
                        <img class="author-profile-image" src="/blog/content/images/size/w100/2025/06/90306c68c41aba2b0c763682af4907f6.png" alt="Dhravya Shah" />
                    </a>
                </li>
                <li class="author-list-item">
                    <a href="/blog/author/soham/" class="author-avatar" aria-label="Read more of Soham Daga">
                        <img class="author-profile-image" src="/blog/content/images/size/w100/2025/06/soham-gc.jpg" alt="Soham Daga" />
                    </a>
                </li>
            </ul>

            <div class="article-byline-meta">
                <h4 class="author-name"><a href="/blog/author/dhravya/">Dhravya Shah</a>, <a href="/blog/author/soham/">Soham Daga</a></h4>
                <div class="byline-meta-content">
                    <time class="byline-meta-date" datetime="2025-06-05">05 Jun 2025</time>
                        <span class="byline-reading-time"><span class="bull">&bull;</span> 5 min read</span>
                </div>
            </div>

        </section>
        </div>

            <figure class="article-image">
                <img
                    srcset="/content/images/size/w300/2025/06/blog-cover.png 300w,
                            /content/images/size/w600/2025/06/blog-cover.png 600w,
                            /content/images/size/w1000/2025/06/blog-cover.png 1000w,
                            /content/images/size/w2000/2025/06/blog-cover.png 2000w"
                    sizes="(min-width: 1400px) 1400px, 92vw"
                    src="/blog/content/images/size/w2000/2025/06/blog-cover.png"
                    alt="Architecting a memory engine inspired by the human brain"
                />
            </figure>

    </header>

    <section class="gh-content gh-canvas">
        <p></p><p>Language is at the heart of intelligence, but what truly powers meaningful interaction is memory ‚Äî the ability to accumulate, recall, and contextualize information over time. </p><p>Large Language Models (LLMs) have mastered language, but memory remains their Achilles‚Äô heel. Every leap in context window size is quickly outpaced by real-world demands: users upload massive documents, have lengthy conversations, and expect seamless recall of preferences and history. The result? LLMs that forget, hallucinate, and frustrate users to the point of wanting to just start over with a clean slate.</p><p>At Supermemory, the idea wasn‚Äôt always to help models remember better. It started off as a way to make *you* remember your bookmarks. <br>Then, it was a cost and accuracy efficient way to easily implement RAG in your product. <br>The more experience we had with this pain point, the more we were led to tackle context, and that‚Äôs what it currently stands as ‚Äî a drop-in memory layer for your LLMs that is extremely scalable, with sub-400ms latency and enterprise-grade reliability.</p><p>Long context does not really solve this either - as found in popular <a href="https://arxiv.org/abs/2502.05167?ref=blog.supermemory.ai" rel="noreferrer">papers </a> and proven time and time again in real-world use-cases.<br><br>But why?</p><h3 id="what-makes-memory-for-llms-so-hard">What Makes Memory for LLMs So Hard?</h3><p>Building a memory layer for AI isn‚Äôt just about one more layer of data storage. You need to optimize for five uncompromising requirements:</p><ul><li><strong>High Recall &amp; Precision:</strong>&nbsp;Always surface the right information ‚Äî even across years of chat history or thousands of documents, and filtering out irrelevant, outdated, or noisy data to keep responses accurate.</li><li><strong>Low Latency:</strong>&nbsp;Memory shouldn‚Äôt slow you down. It needs to work fast, especially at scale.<br><br>This is particularly a challenge - because all current solutions for memory are <em>not</em> built for scale.<ul><li><em>Vector Databases</em>: Either get too expensive, or too slow as they grow. There are new, proprietary, server-less options now - but we'll get into this shortly</li><li><em>Graph</em>: To add every node, or for every query, one typically has to traverse factors or magnitudes more edges than nodes.</li><li><em>Key-value:</em> The entire KV pair has to fit inside the context length of models. Which moves the problem from one context length to another.</li></ul></li><li><strong>Ease of Integration:</strong>&nbsp;Developers need APIs and SDKs that require minimal changes to integrate ‚Äî not weeks of onboarding or complex migrations.<br><br>Managing embeddings, migrating between them, doing research for new improvements, etc. is usually not the primary business goal for apps needing memory. Lots of engineering hours are wasted.</li><li><strong>Semantic &amp; Non-Literal Queries:</strong>&nbsp;Memory must understand nuances, metaphors, and ambiguity ‚Äî not just literal matches. Humans don‚Äôt search against a corpus of data with search terms they kinda know.<br><br>What is "Non-literal match", here's an example</li></ul><figure class="kg-card kg-image-card"><img src="https://supermemory.ai/blog/content/images/2025/06/image.png" class="kg-image" alt="" loading="lazy" width="1504" height="1030" srcset="https://supermemory.ai/blog/content/images/size/w600/2025/06/image.png 600w, https://supermemory.ai/blog/content/images/size/w1000/2025/06/image.png 1000w, https://supermemory.ai/blog/content/images/2025/06/image.png 1504w" sizes="(min-width: 720px) 720px"></figure><p>Most solutions optimize for some of these, but fall short on others ‚Äî especially when it comes to semantic understanding and scaling to billions of data points - For example, a user asking questions about all their internet life may ask more ‚Äúgeneral‚Äù questions that require knowledge of the entire dataset, not just ability to fuzzy search keywords.</p><h2 id="the-supermemory-approach-human-memory-at-scale">The Supermemory Approach: Human Memory at Scale</h2><p>Your brain doesn't store everything perfectly as you see it‚Äîand that's actually a feature, not a bug. It forgets the mundane, emphasizes what you've used recently, and rewrites memories based on current context. Our architecture works the same way, but engineered for AI at scale.</p><p><strong>Smart Forgetting &amp; Decay</strong></p><p>Just like you naturally forget where you parked three weeks ago but remember yesterday's important meeting, our system applies intelligent decay. Less relevant information gradually fades while important, frequently-accessed content stays sharp. No more drowning in irrelevant context.</p><p><strong>Recency &amp; Relevance Bias</strong></p><p>That thing you just talked about? It gets priority. That document you reference constantly? It stays top-of-mind. We mirror your brain's natural tendency to surface what's actually useful right now, not just what's technically "relevant" to a search query.</p><p><strong>Context Rewriting &amp; Broad Connections</strong></p><p>Your brain doesn't just file away facts‚Äîit rewrites them based on new experiences and draws unexpected connections. Our system does the same, continuously updating summaries and finding links between seemingly unrelated information. That random insight from last month might be exactly what you need for today's problem.</p><p><strong>Hierarchical Memory Layers</strong></p><p>Like how you have working memory, short-term memory, and long-term storage, we use Cloudflare's infrastructure to create memory layers that match how you actually think. Hot, recent stuff stays instantly accessible (we personally use <a href="https://developers.cloudflare.com/kv/?ref=blog.supermemory.ai" rel="noreferrer">KV</a>). Deeper memories get retrieved when you need them, not before.</p><h2 id="building-on-top-of-the-engine">Building on top of the engine</h2><p>Supermemory isn‚Äôt just another vector database or RAG toolkit. It‚Äôs a universal memory layer that gives your LLMs the power of infinite context, with near-instant plug-and-play integration. Add our endpoint to your existing AI provider, plug in your API key, and boom ‚Äî you‚Äôre done.</p><p>On top of this engine, we've been building some interesting products and experiences -</p><ul><li><strong>Memory as a service: </strong>Storing and querying multimodal data, at scale, with support for external connectors and sync with Google Drive, Notion, OneDrive, etc. It's a few API calls - just /add, /connect, /search <a href="https://docs.supermemory.ai/api-reference/manage-memories/add-memory?ref=blog.supermemory.ai">https://docs.supermemory.ai/api-reference/manage-memories/add-memory</a> </li></ul><figure class="kg-card kg-image-card"><img src="https://supermemory.ai/blog/content/images/2025/06/SM-2-gradient.png" class="kg-image" alt="" loading="lazy" width="1280" height="720" srcset="https://supermemory.ai/blog/content/images/size/w600/2025/06/SM-2-gradient.png 600w, https://supermemory.ai/blog/content/images/size/w1000/2025/06/SM-2-gradient.png 1000w, https://supermemory.ai/blog/content/images/2025/06/SM-2-gradient.png 1280w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>The Supermemory MCP: </strong> Model-interoperable MCP server that lets users carry their memories, and chats, through LLM apps without losing context. This is actually built on top of our own Memory as a service</li></ul><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">üí• Supermemory MCP can now be used on Claude dot ai!<br><br>Now you can ideate on chat apps, <br>add things to your universal memory <br>...and then use them in your code editors.<br><br>it's YOUR portable memory. <a href="https://t.co/4ju054uQz4?ref=blog.supermemory.ai">https://t.co/4ju054uQz4</a></p>‚Äî supermemory (@Supermemoryai) <a href="https://twitter.com/Supermemoryai/status/1930465720325771420?ref_src=twsrc%5Etfw&ref=blog.supermemory.ai">June 5, 2025</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p></p><p>And our latest launch, called Infinite Chat API, manages memories inline with the conversation history to only send what's needed to the model providers. Leading to less token usage, cost savings, better latencies and better quality responses. <br>You heard that right. You can use it <em>today</em> with just <a href="https://docs.supermemory.ai/infinite-chat?ref=blog.supermemory.ai">one line of code!</a></p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing: Infinite Chat API üí•<br><br>Extend the context length of ANY model <br>...while saving 90% of your tokens and cost AND improving performance!<br><br>One single line to switch. Available to use now <a href="https://t.co/hDfAuu2skD?ref=blog.supermemory.ai">pic.twitter.com/hDfAuu2skD</a></p>‚Äî supermemory (@Supermemoryai) <a href="https://twitter.com/Supermemoryai/status/1923122703009186217?ref_src=twsrc%5Etfw&ref=blog.supermemory.ai">May 15, 2025</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p>We've been constantly improving this with our latest benchmarks, work and progress in the field.</p><figure class="kg-card kg-image-card"><img src="https://supermemory.ai/blog/content/images/2025/06/image-1.png" class="kg-image" alt="" loading="lazy" width="2000" height="1333" srcset="https://supermemory.ai/blog/content/images/size/w600/2025/06/image-1.png 600w, https://supermemory.ai/blog/content/images/size/w1000/2025/06/image-1.png 1000w, https://supermemory.ai/blog/content/images/size/w1600/2025/06/image-1.png 1600w, https://supermemory.ai/blog/content/images/2025/06/image-1.png 2400w" sizes="(min-width: 720px) 720px"></figure><p></p><p></p><p>Memory is a huge missing piece on the road to AGI. With Supermemory, you can finally build products that remember, reason, and respond like never before.</p><p>If you believe in the mission, <a href="https://x.com/i/jobs/1928194391946186862?ref=blog.supermemory.ai">we're hiring</a>. If you want better memory for your LLMs and apps - you can use all these products <em>today</em>. Check out our <a href="https://docs.supermemory.ai/?ref=blog.supermemory.ai">docs</a>, <a href="https://mcp.supermemory.ai/?ref=blog.supermemory.ai" rel="noreferrer">MCP</a>, and get an API key on our <a href="https://console.supermemory.ai/?ref=blog.supermemory.ai" rel="noreferrer">dashboard</a></p>
    </section>


</article>
</main>




            <aside class="read-more-wrap outer">
                <div class="read-more inner">
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/Matryoshka-Representation-Learning.png 300w,
                    /content/images/size/w600/2025/10/Matryoshka-Representation-Learning.png 600w,
                    /content/images/size/w1000/2025/10/Matryoshka-Representation-Learning.png 1000w,
                    /content/images/size/w2000/2025/10/Matryoshka-Representation-Learning.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/Matryoshka-Representation-Learning.png"
            alt="Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It
                </h2>
            </header>
                <div class="post-card-excerpt">Embeddings are the cornerstone of any retrieval system. And the larger the embeddings, the more information they can store.

But large embeddings require a lot of memory, which leads to high computational costs and latency.

To reduce this high cost, we can use models that produce embeddings with small dimensions,</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-19">19 Oct 2025</time>
                <span class="post-card-meta-length">8 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/incident-report-october-18-2025-service-degradation/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/Frame-2147228224.png 300w,
                    /content/images/size/w600/2025/10/Frame-2147228224.png 600w,
                    /content/images/size/w1000/2025/10/Frame-2147228224.png 1000w,
                    /content/images/size/w2000/2025/10/Frame-2147228224.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/Frame-2147228224.png"
            alt="Incident Report: October 18, 2025 Service Degradation"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/incident-report-october-18-2025-service-degradation/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    Incident Report: October 18, 2025 Service Degradation
                </h2>
            </header>
                <div class="post-card-excerpt">Summary

On October 18, between 1:17 PM and 1:45 PM PDT, we experienced service degradation that resulted in elevated API response times and some timeouts. This happened when two enterprise customers started major data backfills simultaneously‚Äî while we&#39;d planned for one, the second caught us by</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-19">19 Oct 2025</time>
                <span class="post-card-meta-length">6 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/how-to-make-your-mcp-clients-share-context-with-supermemory-mcp/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/18.png 300w,
                    /content/images/size/w600/2025/10/18.png 600w,
                    /content/images/size/w1000/2025/10/18.png 1000w,
                    /content/images/size/w2000/2025/10/18.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/18.png"
            alt="How To Make Your MCP Clients Share Context with Supermemory MCP"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/how-to-make-your-mcp-clients-share-context-with-supermemory-mcp/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    How To Make Your MCP Clients Share Context with Supermemory MCP
                </h2>
            </header>
                <div class="post-card-excerpt">Let‚Äôs get practical here: have you ever dropped a PDF into Cursor, then pasted the same content into Claude just to ‚Äúremind it‚Äù? Or tried to follow up on a thread, only to realize the memory lives in a different tool?

It‚Äôs annoying. It breaks your flow. And</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-07">07 Oct 2025</time>
                <span class="post-card-meta-length">5 min read</span>
        </footer>

    </div>

</article>
                </div>
            </aside>



    </div>

    <footer class="site-footer outer">
        <div class="inner">
            <section class="copyright"><a href="https://supermemory.ai/blog">supermemory - Blog</a> &copy; 2025</section>
            <nav class="site-footer-nav">
                <ul class="nav">
    <li class="nav-sign-up"><a href="#/portal/">Sign up</a></li>
    <li class="nav-get-started"><a href="https://console.supermemory.ai">Get Started</a></li>
</ul>

            </nav>
            <div class="gh-powered-by"><a href="https://ghost.org/" target="_blank" rel="noopener">Powered by Ghost</a></div>
        </div>
    </footer>

</div>

    <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="pswp__bg"></div>

    <div class="pswp__scroll-wrap">
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script
    src="https://code.jquery.com/jquery-3.5.1.min.js"
    integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous">
</script>
<script src="/blog/assets/built/casper.js?v=259661198b"></script>
<script>
$(document).ready(function () {
    // Mobile Menu Trigger
    $('.gh-burger').click(function () {
        $('body').toggleClass('gh-head-open');
    });
    // FitVids - Makes video embeds responsive
    $(".gh-content").fitVids();
});
</script>

<script>
  // Change main logo link
  const mainLogo = document.querySelector('a.gh-head-logo');
  if (mainLogo) {
    mainLogo.href = "https://supermemory.ai/";
  }

  // Add "Get Started" button to gh-head-actions
  const actionsDiv = document.querySelector('div.gh-head-actions');
  if (actionsDiv) {
    const btn = document.createElement('a');
    btn.href = "https://console.supermemory.ai";
    btn.textContent = "Get Started";

    // Button styles
    btn.style.background = "#267BF1";
    btn.style.color = "#FFF";
    btn.style.padding = "1rem 2rem";
    btn.style.borderRadius = "6px";
    btn.style.fontWeight = "600";
    btn.style.textDecoration = "none";
    btn.style.fontSize = "1.6rem";
    btn.style.transition = "background 0.2s";
    btn.onmouseover = () => btn.style.background = "#1563c7";
    btn.onmouseout = () => btn.style.background = "#267BF1";

    actionsDiv.appendChild(btn);
  }
</script>

</body>
</html>
