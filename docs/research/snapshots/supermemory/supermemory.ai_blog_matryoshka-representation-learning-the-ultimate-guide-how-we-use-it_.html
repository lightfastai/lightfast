<!DOCTYPE html>
<html lang="en" class="dark-mode">
<head>
<meta name="robots" content="index, follow">

    <title>Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link rel="preload" as="style" href="/blog/assets/built/screen.css?v=259661198b" />
    <link rel="preload" as="script" href="/blog/assets/built/casper.js?v=259661198b" />

    <link rel="stylesheet" type="text/css" href="/blog/assets/built/screen.css?v=259661198b" />

    <link rel="icon" href="https://supermemory.ai/blog/content/images/size/w256h256/2025/06/SuperM_LinkedIn-Github-Twitter_ProfilePicture--1--1.png" type="image/png">
    <link rel="canonical" href="https://supermemory.ai/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="supermemory - Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It">
    <meta property="og:description" content="Embeddings are the cornerstone of any retrieval system. And the larger the embeddings, the more information they can store.

But large embeddings require a lot of memory, which leads to high computational costs and latency.

To reduce this high cost, we can use models that produce embeddings with small dimensions,">
    <meta property="og:url" content="https://supermemory.ai/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/">
    <meta property="og:image" content="https://supermemory.ai/blog/content/images/2025/10/Matryoshka-Representation-Learning.png">
    <meta property="article:published_time" content="2025-10-19T17:44:20.000Z">
    <meta property="article:modified_time" content="2025-10-19T17:44:20.000Z">
    <meta property="article:tag" content="Learning">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It">
    <meta name="twitter:description" content="Embeddings are the cornerstone of any retrieval system. And the larger the embeddings, the more information they can store.

But large embeddings require a lot of memory, which leads to high computational costs and latency.

To reduce this high cost, we can use models that produce embeddings with small dimensions,">
    <meta name="twitter:url" content="https://supermemory.ai/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/">
    <meta name="twitter:image" content="https://supermemory.ai/blog/content/images/2025/10/Matryoshka-Representation-Learning.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Naman Bansal">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Learning">
    <meta name="twitter:site" content="@supermemoryai">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="675">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "supermemory - Blog",
        "url": "https://supermemory.ai/blog/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://supermemory.ai/blog/content/images/2025/06/Frame-2147223248.svg"
        }
    },
    "author": {
        "@type": "Person",
        "name": "Naman Bansal",
        "image": {
            "@type": "ImageObject",
            "url": "https://www.gravatar.com/avatar/1b424ffbaa308b371e62efa5919dfe3d?s=250&r=x&d=mp",
            "width": 250,
            "height": 250
        },
        "url": "https://supermemory.ai/blog/author/naman/",
        "sameAs": []
    },
    "headline": "Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It",
    "url": "https://supermemory.ai/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/",
    "datePublished": "2025-10-19T17:44:20.000Z",
    "dateModified": "2025-10-19T17:44:20.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://supermemory.ai/blog/content/images/2025/10/Matryoshka-Representation-Learning.png",
        "width": 1200,
        "height": 675
    },
    "keywords": "Learning",
    "description": "Embeddings are the cornerstone of any retrieval system. And the larger the embeddings, the more information they can store.\n\nBut large embeddings require a lot of memory, which leads to high computational costs and latency.\n\nTo reduce this high cost, we can use models that produce embeddings with small dimensions, but even that has a bunch of drawbacks. Smaller embeddings usually can&#x27;t encapsulate as much semantic information as their larger counterparts.\n\nFor example, bert-base-uncased with 768",
    "mainEntityOfPage": "https://supermemory.ai/blog/matryoshka-representation-learning-the-ultimate-guide-how-we-use-it/"
}
    </script>

    <meta name="generator" content="Ghost 5.130">
    <link rel="alternate" type="application/rss+xml" title="supermemory - Blog" href="https://supermemory.ai/blog/rss/">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.8/umd/sodo-search.min.js" data-key="d2a094c14f6148bdbd8ad26051" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.8/umd/main.css" data-sodo-search="https://supermemory.ai/blog/" data-locale="en" crossorigin="anonymous"></script>
    
    <link href="https://supermemory.ai/blog/webmentions/receive/" rel="webmention">
    <script defer src="/blog/public/cards.min.js?v=259661198b"></script><style>:root {--ghost-accent-color: #3d49d8;}</style>
    <link rel="stylesheet" type="text/css" href="/blog/public/cards.min.css?v=259661198b">
    <style>
  .gh-footer-copyright {
    display: none !important;
}
a[href*="ghost.org"] {
    display: none !important;
}
::selection {
  background: #267BF1;
  color: #FFF;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
    // Find all navigation logo links
    const logoLinks = document.querySelectorAll('.gh-navigation-logo');
    
    logoLinks.forEach(function(link) {
        // Change the href to point to main site
        link.href = 'https://supermemory.ai';
    });
});

  // Ensure all pages point to the main domain version
  const canonical = document.querySelector('link[rel="canonical"]');
  if (canonical && canonical.href.includes('blog.supermemory.ai')) {
    canonical.href = canonical.href.replace('blog.supermemory.ai', 'supermemory.ai/blog');
  }

if (typeof window !== 'undefined') {
  // Client-side check
  const hostname = window.location.hostname;
  const userAgent = navigator.userAgent;
  const isCloudflareWorker = userAgent.includes('Cloudflare-Workers');
  const isGhostAdmin = window.location.pathname.startsWith('/ghost');
  
  if (hostname === 'blog.supermemory.ai' && !isCloudflareWorker && !isGhostAdmin) {
    window.location.replace('https://supermemory.ai/blog' + window.location.pathname.replace('/blog', '') + window.location.search);
  }
}
</script>

<script>
    !function(t,e){var o,n,p,r;e.__SV||(window.posthog && window.posthog.__loaded)||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init Ce js Ls Te Fs Ds capture Ye calculateEventProperties zs register register_once register_for_session unregister unregister_for_session Ws getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey displaySurvey canRenderSurvey canRenderSurveyAsync identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty Bs Us createPersonProfile Hs Ms Gs opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing get_explicit_consent_status is_capturing clear_opt_in_out_capturing Ns debug L qs getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
    posthog.init('phc_9wkqAZtZYAUCNwvus0hYqcZbw5EBEX2s3QXjZoNdUNS', {
        api_host: 'https://us.i.posthog.com',
        defaults: '2025-05-24',
        person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
    })
</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
    <link rel="preconnect" href="https://fonts.bunny.net"><link rel="stylesheet" href="https://fonts.bunny.net/css?family=space-grotesk:700|space-mono:400,700"><style>:root {--gh-font-heading: Space Grotesk;--gh-font-body: Space Mono;}</style>

</head>
<body class="post-template tag-learning gh-font-heading-space-grotesk gh-font-body-space-mono is-head-left-logo">
<div class="viewport">

    <header id="gh-head" class="gh-head outer">
        <div class="gh-head-inner inner">
            <div class="gh-head-brand">
                <a class="gh-head-logo" href="https://supermemory.ai/blog">
                        <img src="https://supermemory.ai/blog/content/images/2025/06/Frame-2147223248.svg" alt="supermemory - Blog">
                </a>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
                <button class="gh-burger" aria-label="Main Menu"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-home"><a href="https://supermemory.ai">Home</a></li>
    <li class="nav-blogs"><a href="https://supermemory.ai/blog">Blogs</a></li>
    <li class="nav-updates"><a href="https://docs.supermemory.ai/changelog/overview">Updates</a></li>
    <li class="nav-docs"><a href="https://docs.supermemory.ai">Docs</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                        <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
            </div>
        </div>
    </header>

    <div class="site-content">
        



<main id="site-main" class="site-main">
<article class="article post tag-learning ">

    <header class="article-header gh-canvas">

        <div class="article-tag post-card-tags">
                <span class="post-card-primary-tag">
                    <a href="/blog/tag/learning/">Learning</a>
                </span>
        </div>

        <h1 class="article-title">Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It</h1>


        <div class="article-byline">
        <section class="article-byline-content">

            <ul class="author-list instapaper_ignore">
                <li class="author-list-item">
                    <a href="/blog/author/naman/" class="author-avatar" aria-label="Read more of Naman Bansal">
                        <img class="author-profile-image" src="https://www.gravatar.com/avatar/1b424ffbaa308b371e62efa5919dfe3d?s&#x3D;250&amp;r&#x3D;x&amp;d&#x3D;mp" alt="Naman Bansal" />
                    </a>
                </li>
            </ul>

            <div class="article-byline-meta">
                <h4 class="author-name"><a href="/blog/author/naman/">Naman Bansal</a></h4>
                <div class="byline-meta-content">
                    <time class="byline-meta-date" datetime="2025-10-19">19 Oct 2025</time>
                        <span class="byline-reading-time"><span class="bull">&bull;</span> 8 min read</span>
                </div>
            </div>

        </section>
        </div>

            <figure class="article-image">
                <img
                    srcset="/content/images/size/w300/2025/10/Matryoshka-Representation-Learning.png 300w,
                            /content/images/size/w600/2025/10/Matryoshka-Representation-Learning.png 600w,
                            /content/images/size/w1000/2025/10/Matryoshka-Representation-Learning.png 1000w,
                            /content/images/size/w2000/2025/10/Matryoshka-Representation-Learning.png 2000w"
                    sizes="(min-width: 1400px) 1400px, 92vw"
                    src="/blog/content/images/size/w2000/2025/10/Matryoshka-Representation-Learning.png"
                    alt="Matryoshka Representation Learning: The Ultimate Guide &amp; How We Use It"
                />
            </figure>

    </header>

    <section class="gh-content gh-canvas">
        <p><a href="https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/?ref=blog.supermemory.ai">Embeddings</a> are the cornerstone of any retrieval system. And the larger the embeddings, the more information they can store.</p><p>But large embeddings require a lot of memory, which leads to high computational costs and latency.</p><p>To reduce this high cost, we can use models that produce embeddings with small dimensions, but even that has a bunch of drawbacks. Smaller embeddings usually can't encapsulate as much semantic information as their larger counterparts.</p><p>For example, <code>bert-base-uncased</code> with <code>768</code> dimensions is not as good as OpenAI's <code>text-embedding-3-large</code> with <code>3072</code> dimensions.</p><p>Thus, we needed a solution that balances this tradeoff between latency/cost and semantic capture, and that is exactly what <strong>Matryoshka Representation Learning</strong> does.</p><p>This article will break down MRL, show you how to implement it, and how it has helped us at Supermemory build our memory engine.</p><h2 id="what-is-matryoshka-representation-learning-mrl">What is Matryoshka Representation Learning (MRL)?</h2><p>Matryoshka Representation Learning is a novel technique used to create vector embeddings with the same model, but with varying sizes.</p><p>Let's elaborate.</p><p>Usually, to create embeddings, we pass text through a model, and we get an embedding of a specific dimension, like 1024. Once you create this embedding, you can't change its dimensions.</p><p>But, in MRL, when we create an embedding, we can create embeddings of various sizes, like 128, 256, 768, etc. Creating smaller embeddings cuts compute and latency, which makes retrieval faster.</p><p>With Matryoshka Representation Learning, we do not train separate models for each dimension. We produce a single full vector and structure it so we can slice it to different sizes. Early dimensions carry the core semantics; later dimensions add finer detail.</p><p>This lets us run shortlisting and reranking efficiently: shortlist with a small prefix, then rerank with a larger slice for accuracy. We’ll dig into shortlisting and reranking next.</p><h3 id="shortlisting-and-reranking">Shortlisting and Reranking</h3><p>As the name suggests, this method is broken down into two main parts. The first one is:</p><ol><li><strong>Shortlisting</strong>:<ul><li>Shortlisting is the process of using small-sized embeddings to retrieve a few top relevant documents very quickly.</li><li>For example, retrieving 200 relevant documents from 1000 documents.</li><li>Here, we trade off accuracy for speed and efficiency.</li></ul></li><li><strong>Reranking</strong>:<ul><li>Here, we use full-sized embeddings to rerank the existing 200 documents.</li><li>For example, D1, D2, D3 can become D2, D3, D1 after reranking.</li><li>Reranking does not affect our efficiency, as we only perform it on a small number of documents compared to the original number of documents.</li></ul></li></ol><p>So, in short:</p><ul><li>First we use MRL to create varying embeddings.</li><li>Then, we use smaller embeddings to retrieve a few top documents related to the query over a large set of documents.</li><li>After that, we rerank or rearrange the retrieved documents.</li></ul><p>Thus, we get the most relevant and important documents for the query.</p><h2 id="how-does-mrl-work">How does MRL work?</h2><p>MRL can create embeddings with varying dimensions. But, how exactly?</p><p>MRL is not an embedding model with a specific structure or something, but it's a method that can be applied to train any existing embedding model. It can even be used to fine-tune pre-trained models.</p><p>Given is the visual representation of how MRL can be applied over any existing embedding strategy:</p><figure class="kg-card kg-image-card"><img src="https://i.ibb.co/Z60kq7C7/Screenshot-2025-10-12-at-3-02-30-PM.png" class="kg-image" alt="figure.png" loading="lazy" width="670" height="560"></figure><p>Before diving into the math and nitty-gritty of MRL, let's understand the core idea of MRL, which is <strong>calculating loss in a bit of a different way</strong>.</p><p>In MRL, you calculate a loss function for various dimensions instead of just one loss function for one fixed dimension size. Then, you add up or take the average of all the loss functions.</p><p>For instance, in a normal embedding model, we would calculate a single loss for a specific dimension like <code>1024</code>.</p><p>But when it comes to MRL, we calculate loss for the original (larger) size of the embedding, but along with it, we also calculate loss for a list of dimensions (the values of these dimensions are a hyper-parameter and are fixed during training) like <code>128</code>, <code>256</code>, <code>764</code>, and <code>1024</code>.</p><p>Once we calculate this loss for each dimension, we then add/average them to get the final single loss value on which we will perform backpropagation to learn embeddings.</p><h2 id="the-math-behind-mrl">The Math Behind MRL</h2><p>Except for the loss, all other processes in MRL are similar to any other embedding models.</p><p>Given is the loss function used by MRL:</p><p>$$<br>
L_{\text{MRL}} = \sum_{m \in M} c_m \cdot L(z_{1:m})<br>
$$</p>
<ul><li>\\(L_{\text{MRL}}\\) is the total Matryoshka loss.</li><li>\\(M\\) is the set of pre-defined dimensions you want to train on (e.g., <code>{64, 128, 256, 768}</code>).</li><li>\\(m\\) is a specific dimension from the set M.</li><li>$c_{m}$ is a weighting factor (relative importance) for the loss at dimension m. This is often set to 1 for all dimensions, giving them equal importance.</li><li>$L$ is your base loss function (e.g., <a href="https://medium.com/@chris.p.hughes10/a-brief-overview-of-cross-entropy-loss-523aa56b75d5?ref=blog.supermemory.ai">Cross-Entropy</a>, <a href="https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246?ref=blog.supermemory.ai">Contrastive Loss</a>).</li><li>$z_{1:m}$ is the embedding vector truncated to its first m dimensions.</li></ul><p>The weighting factor is used to give importance to a specific dimension size. Usually, it's set at 1 so that we give equal importance to all the dimensions and the model can learn a better representation at all dimension levels.</p><p>But for specific tasks, for instance extremely fast retrieval and low accuracy, we can use a weighting factor to give smaller dimension sizes more importance.</p><p>For example: $c_{128}=2.0$, $c_{256}​=1.0$, $c_{768}​=0.5$</p><p><em>A simpler formula for intuition:</em></p><p>$$<br>Loss = loss_{32} + loss_{64} + loss_{128} + loss_{256} + loss_{1024}<br>$$</p><p><em>A complex formula for no reason (given in the original research paper):</em></p><figure class="kg-card kg-image-card"><img src="https://i.ibb.co/nT1nBr6/Screenshot-2025-10-12-at-3-06-51-PM.png" class="kg-image" alt="loss_funk.png" loading="lazy" width="1302" height="580"></figure><h2 id="standard-way-vs-matryoshka-way-of-training">Standard Way vs. Matryoshka Way of Training</h2><p><strong>Standard Embedding Training</strong></p><ol><li>Take input data (e.g., a sentence).</li><li>Pass it through the model to get a single, full-sized embedding (e.g., 768 dimensions).</li><li>Calculate <strong>one loss value</strong> based on how well that 768-dim embedding performs on the task (e.g., a contrastive loss in Sentence Transformers).</li><li>The optimizer updates the model's weights based on that single loss.</li></ol><p><strong>Matryoshka Representation Learning (MRL) Training</strong></p><ol><li>Take input data.</li><li>Pass it through the model to get the single, full-sized embedding.</li><li>Calculate <strong>multiple losses</strong> in parallel:<ul><li><strong>Loss 1:</strong> On the full embedding (<code>vector[:]</code>).</li><li><strong>Loss 2:</strong> On a truncated prefix of the embedding (<code>vector[:512]</code>).</li><li><strong>Loss 3:</strong> On an even shorter prefix (<code>vector[:256]</code>).</li><li><strong>Loss 4:</strong> On the shortest prefix (<code>vector[:64]</code>).</li></ul></li><li>These individual loss values are then <strong>aggregated</strong> (e.g., summed or averaged) into a single, final loss value.</li><li>The optimizer updates the model's weights based on this <strong>single aggregated loss</strong>.</li></ol><h2 id="important-interesting-notes">Important, interesting notes</h2><p>So, why the name 'Matryoshka'?</p><p>The name comes from Russian Matryoshka dolls, where smaller dolls are nested inside larger ones.</p><p>MRL works the same way with embeddings:</p><ul><li>The 128-dimension embedding is not just a separate, smaller embedding; it's literally the <strong>first 128 values</strong> of the larger 256-dimension embedding.</li><li>The 256-dimension embedding is the <strong>first 256 values</strong> of the 768-dimension one, and so on.</li></ul><figure class="kg-card kg-image-card"><img src="https://i.ibb.co/6RLFB1BM/Russian-Matroshka.jpg" class="kg-image" alt="dolls.png" loading="lazy" width="2560" height="1920"></figure><p>MRL creates an embedding in such a way that our first few dimensions hold the semantic information with the highest value, i.e., they can explain the main idea or theme behind the text, or they encapsulate the most information about the text. The latter dimensions enrich that information with finer details.</p><h2 id="code-implementation">Code Implementation</h2><p>Let's look at the code of how we can implement MRL using the pre-trained <em>MPnet base model</em>,</p><pre><code class="language-python">import torch
from sentence_transformers import SentenceTransformer, util

model_name = 'tomaarsen/mpnet-base-nli-matryoshka'
model = SentenceTransformer(model_name)

source_sentence = "A person is riding a motorcycle on a road."
similar_sentence = "A man is on a motorbike on a street."
dissimilar_sentence = "The cat is sleeping on the kitchen table."

sentences = [source_sentence, similar_sentence, dissimilar_sentence]
  
full_embeddings = model.encode(sentences, convert_to_tensor=True)

matryoshka_dims = [64, 128, 256, 512, 768] # these are the different embeddings for which loss will be calculated and aggregated.

print(f"--- Using Model: '{model_name}' ---")
print(f"Full embedding dimension: {full_embeddings.shape[1]}")

for dim in matryoshka_dims:
	print(f"\n--- Testing with dimension: {dim} ---")
	
	sub_embeddings = full_embeddings[:, :dim]
	
	scores = util.cos_sim(sub_embeddings[0], sub_embeddings[1:])
	
	score_similar = scores[0][0]
	score_dissimilar = scores[0][1]
	
	print(f"Similarity with '{similar_sentence}': {score_similar.item():.4f}")
	print(f"Similarity with '{dissimilar_sentence}':{score_dissimilar.item():.4f}")
</code></pre><p><strong>Output:</strong></p><pre><code>--- Using Model: 'tomaarsen/mpnet-base-nli-matryoshka' --- 
Full embedding dimension: 768 

--- Testing with dimension: 64 --- 
Similarity with 'A man is on a motorbike on a street.': 0.7775 
Similarity with 'The cat is sleeping on the kitchen table.': -0.0945 

--- Testing with dimension: 128 --- 
Similarity with 'A man is on a motorbike on a street.': 0.7687 
Similarity with 'The cat is sleeping on the kitchen table.': -0.0744 

--- Testing with dimension: 256 --- 
Similarity with 'A man is on a motorbike on a street.': 0.7727 
Similarity with 'The cat is sleeping on the kitchen table.': -0.0648 

--- Testing with dimension: 512 --- 
Similarity with 'A man is on a motorbike on a street.': 0.7630 
Similarity with 'The cat is sleeping on the kitchen table.': -0.0106 

--- Testing with dimension: 768 --- 
Similarity with 'A man is on a motorbike on a street.': 0.7516 
Similarity with 'The cat is sleeping on the kitchen table.': 0.0294
</code></pre><p>Here we initialize <em>MPnet base model</em> using the <code>SentenceTransformer()</code> library. Then we create a source sentence and two more example sentences, such that one is similar to the source and the other is dissimilar.</p><p>The motive behind creating these two sentences is to check the similarity of the source with both of them. We compute the similarity score for all the Matryoshka dimensions. Sentences are then embedded using an embedding model (one which is specifically trained with the Matryoshka loss method).</p><p>We use a for loop to iterate over all the dimensions in <code>matryoshka_dims</code>, and for each dimension, we slice the full embedding into a smaller size (same as the dimension).</p><p>These smaller embeddings are then used to calculate the similarity score between the source sentence and the example sentences (both similar and dissimilar ones) using <a href="https://en.wikipedia.org/wiki/Cosine_similarity?ref=blog.supermemory.ai">Cosine Similarity</a>.</p><p>In the end, we print the similarity scores for each embedding dimension so that we can compare how each embedding performs.</p><h2 id="how-we-use-mrl-at-supermemory-and-our-learnings">How we use MRL at Supermemory and our learnings</h2><p>We started with full-length provider embeddings, around 3k dims in our case, and hit the usual pain points: big indexes, slower queries, and rising storage.</p><p>After reading Exa’s write-up on billion-scale storage, we tried Matryoshka-style embeddings and kept them in production.</p><p>What we do uniquely, which I think a lot of other people don't know is, not just slicing, but also normalization of the embeddings and quantizing them.</p><h3 id="normalization">Normalization</h3><p>Prefix slices live in a smaller subspace. If you use them raw, vectors spread out more than they should and cosine scores get jumpy. We L2-normalize embeddings so that a 64-, 128-, or 256-dim slice sits in a stable geometry.</p><p>We compute the sum of squares, take the square root, and divide the vector by that norm. We apply the same idea to prefixes, so a 128-dim slice is scaled in proportion to the part of the space we are using.</p><p>This simple step tightened precision and pulled in items that were previously missed. It also increased separation where it mattered, pushing clearly unrelated pairs further apart.</p><h3 id="quantization">Quantization.</h3><p>After normalization, we apply lightweight quantization. Storage drops a lot without a noticeable quality hit for our workload. We still keep the full embedding, and we also keep a compact view for fast paths.</p><p>Storage goes up a bit overall, but index size and query speed improve where it counts.</p><h3 id="quality-vs-dimension">Quality vs dimension.</h3><p>Empirically, halving the dimension kept quality near intact for us, and going down to roughly one third still preserved most of it. That let us shortlist on small prefixes and reserve the full vector for rerank.</p><h3 id="provider-tips">Provider tips.</h3><p>If the API lets you request a target dimension, ask for it directly. Smaller outputs come back faster. Some providers also return those smaller vectors pre-normalized. In our tests, Voyage does; Google does not.</p><p>The main tradeoff is that slicing can lose certain latent interactions that only exist in the full space.</p><p>That tradeoff is worth it in practice for us, and it is one reason our memory lookups are fast in production, with end-to-end retrieval and rerank routinely under 200 ms.</p><p>If you want to add real memory instead of a vector dump to your LLMs and agents, <a>try Supermemory</a> with your data and see the difference in recall and response quality.</p><p>Get started in <a>just 5 mins.</a></p>
    </section>


</article>
</main>




            <aside class="read-more-wrap outer">
                <div class="read-more inner">
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/incident-report-october-18-2025-service-degradation/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/Frame-2147228224.png 300w,
                    /content/images/size/w600/2025/10/Frame-2147228224.png 600w,
                    /content/images/size/w1000/2025/10/Frame-2147228224.png 1000w,
                    /content/images/size/w2000/2025/10/Frame-2147228224.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/Frame-2147228224.png"
            alt="Incident Report: October 18, 2025 Service Degradation"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/incident-report-october-18-2025-service-degradation/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    Incident Report: October 18, 2025 Service Degradation
                </h2>
            </header>
                <div class="post-card-excerpt">Summary

On October 18, between 1:17 PM and 1:45 PM PDT, we experienced service degradation that resulted in elevated API response times and some timeouts. This happened when two enterprise customers started major data backfills simultaneously— while we&#39;d planned for one, the second caught us by</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-19">19 Oct 2025</time>
                <span class="post-card-meta-length">6 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post keep-ratio">

    <a class="post-card-image-link" href="/blog/how-to-make-your-mcp-clients-share-context-with-supermemory-mcp/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/18.png 300w,
                    /content/images/size/w600/2025/10/18.png 600w,
                    /content/images/size/w1000/2025/10/18.png 1000w,
                    /content/images/size/w2000/2025/10/18.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/18.png"
            alt="How To Make Your MCP Clients Share Context with Supermemory MCP"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/how-to-make-your-mcp-clients-share-context-with-supermemory-mcp/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    How To Make Your MCP Clients Share Context with Supermemory MCP
                </h2>
            </header>
                <div class="post-card-excerpt">Let’s get practical here: have you ever dropped a PDF into Cursor, then pasted the same content into Claude just to “remind it”? Or tried to follow up on a thread, only to realize the memory lives in a different tool?

It’s annoying. It breaks your flow. And</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-07">07 Oct 2025</time>
                <span class="post-card-meta-length">5 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post featured keep-ratio">

    <a class="post-card-image-link" href="/blog/supermemory-raises-3-million-and-building-the-best-memory-engine-for-llms/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2025/10/Announcement-3.png 300w,
                    /content/images/size/w600/2025/10/Announcement-3.png 600w,
                    /content/images/size/w1000/2025/10/Announcement-3.png 1000w,
                    /content/images/size/w2000/2025/10/Announcement-3.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/blog/content/images/size/w600/2025/10/Announcement-3.png"
            alt="Supermemory raises $3 million with the best memory engine for LLMs"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/blog/supermemory-raises-3-million-and-building-the-best-memory-engine-for-llms/">
            <header class="post-card-header">
                <div class="post-card-tags">
                        <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
                </div>
                <h2 class="post-card-title">
                    Supermemory raises $3 million with the best memory engine for LLMs
                </h2>
            </header>
                <div class="post-card-excerpt">Today, I am excited to announce our first funding round to accelerate our mission of building an interoperable, scalable and reliable memory for LLMs and agents.

Memory is one of the hardest challenges in AI right now. We have really intelligent models (Claude, GPT-5, etc) and tools (Cursor, and many</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2025-10-06">06 Oct 2025</time>
                <span class="post-card-meta-length">2 min read</span>
        </footer>

    </div>

</article>
                </div>
            </aside>



    </div>

    <footer class="site-footer outer">
        <div class="inner">
            <section class="copyright"><a href="https://supermemory.ai/blog">supermemory - Blog</a> &copy; 2025</section>
            <nav class="site-footer-nav">
                <ul class="nav">
    <li class="nav-sign-up"><a href="#/portal/">Sign up</a></li>
    <li class="nav-get-started"><a href="https://console.supermemory.ai">Get Started</a></li>
</ul>

            </nav>
            <div class="gh-powered-by"><a href="https://ghost.org/" target="_blank" rel="noopener">Powered by Ghost</a></div>
        </div>
    </footer>

</div>

    <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="pswp__bg"></div>

    <div class="pswp__scroll-wrap">
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script
    src="https://code.jquery.com/jquery-3.5.1.min.js"
    integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous">
</script>
<script src="/blog/assets/built/casper.js?v=259661198b"></script>
<script>
$(document).ready(function () {
    // Mobile Menu Trigger
    $('.gh-burger').click(function () {
        $('body').toggleClass('gh-head-open');
    });
    // FitVids - Makes video embeds responsive
    $(".gh-content").fitVids();
});
</script>

<script>
  // Change main logo link
  const mainLogo = document.querySelector('a.gh-head-logo');
  if (mainLogo) {
    mainLogo.href = "https://supermemory.ai/";
  }

  // Add "Get Started" button to gh-head-actions
  const actionsDiv = document.querySelector('div.gh-head-actions');
  if (actionsDiv) {
    const btn = document.createElement('a');
    btn.href = "https://console.supermemory.ai";
    btn.textContent = "Get Started";

    // Button styles
    btn.style.background = "#267BF1";
    btn.style.color = "#FFF";
    btn.style.padding = "1rem 2rem";
    btn.style.borderRadius = "6px";
    btn.style.fontWeight = "600";
    btn.style.textDecoration = "none";
    btn.style.fontSize = "1.6rem";
    btn.style.transition = "background 0.2s";
    btn.onmouseover = () => btn.style.background = "#1563c7";
    btn.onmouseout = () => btn.style.background = "#267BF1";

    actionsDiv.appendChild(btn);
  }
</script>

</body>
</html>
